{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec33d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/api-visn/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/merged_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['employment_type'].replace({'Full Time': 'Full-time', 'FULL_TIME': 'Full-time', 'Fulltime': 'Full-time', 'FT': 'Full-time',\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['loan_type'].replace({'Personal Loan': 'Personal', 'personal': 'Personal', 'PERSONAL': 'Personal',\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['loan_purpose'].replace({'Mortgage': 'Home Loan'}, inplace=True)\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['num_delinquencies_2yrs'].fillna(0, inplace=True)\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['employment_length'].fillna(df['employment_length'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/merged_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['employment_type'].replace({'Full Time': 'Full-time', 'FULL_TIME': 'Full-time', 'Fulltime': 'Full-time', 'FT': 'Full-time',\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['loan_type'].replace({'Personal Loan': 'Personal', 'personal': 'Personal', 'PERSONAL': 'Personal',\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['loan_purpose'].replace({'Mortgage': 'Home Loan'}, inplace=True)\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:37: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['num_delinquencies_2yrs'].fillna(0, inplace=True)\n",
      "/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/source/data_pipeline.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['employment_length'].fillna(df['employment_length'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 67  Categorical features: 0\n",
      "Training pipeline ...\n",
      "[LightGBM] [Info] Number of positive: 3675, number of negative: 68324\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008533 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 6492\n",
      "[LightGBM] [Info] Number of data points in the train set: 71999, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Test AUC: 0.7917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95     17081\n",
      "           1       0.26      0.35      0.29       919\n",
      "\n",
      "    accuracy                           0.92     18000\n",
      "   macro avg       0.61      0.65      0.62     18000\n",
      "weighted avg       0.93      0.92      0.92     18000\n",
      "\n",
      "Saved pipeline to final_lgbm.pkl\n",
      "Saved preprocessor to preprocessor.pkl\n",
      "Computing SHAP values (may take some time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/api-visn/lib/python3.12/site-packages/shap/explainers/_tree.py:587: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SHAP summary to shap_summary.png\n",
      "Saved aggregated SHAP importances to shap_aggregated.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "credit_pipeline.py\n",
    "Full training pipeline: preprocessing -> PCA -> SMOTEENN -> LightGBM\n",
    "Includes SHAP explainability and saving artifacts.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration\n",
    "# -------------------------------\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "PCA_VARIANCE = 0.95  # keep 95% variance\n",
    "MODEL_OUTPUT = \"final_lgbm.pkl\"\n",
    "PREPROCESSOR_OUTPUT = \"preprocessor.pkl\"\n",
    "SHAP_PLOT_OUTPUT = \"shap_summary.png\"\n",
    "\n",
    "# -------------------------------\n",
    "# Helper: load your data here\n",
    "# -------------------------------\n",
    "# Replace this with actual loading; expects df with columns and 'default' target\n",
    "# Example:\n",
    "# df = pd.read_csv(\"data/credit_data.csv\")\n",
    "\n",
    "# For testing purpose raise if df not defined\n",
    "from data_pipeline import load_data\n",
    "try:\n",
    "    df = load_data()\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Please set `df` variable to your pandas DataFrame before running the script.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Features & target\n",
    "# -------------------------------\n",
    "TARGET = 'default'\n",
    "features = [c for c in df.columns if c != TARGET]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[TARGET].astype(int).copy()\n",
    "\n",
    "# -------------------------------\n",
    "# Identify numeric / categorical columns\n",
    "# -------------------------------\n",
    "num_features = X.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_features = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Remove target-col-like from features if erroneously included\n",
    "if TARGET in num_features:\n",
    "    num_features.remove(TARGET)\n",
    "if TARGET in cat_features:\n",
    "    cat_features.remove(TARGET)\n",
    "\n",
    "print(f\"Numeric features: {len(num_features)}  Categorical features: {len(cat_features)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Train/test split (important: do it BEFORE resampling)\n",
    "# -------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Preprocessor\n",
    "# -------------------------------\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_features),\n",
    "        # (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Build imbalanced pipeline (preprocess -> PCA -> SMOTEENN -> LGBM)\n",
    "# Note: Use imblearn Pipeline so resampling step is supported\n",
    "# -------------------------------\n",
    "pca = PCA(n_components=PCA_VARIANCE, svd_solver=\"full\")\n",
    "resampler = SMOTEENN(random_state=RANDOM_STATE)\n",
    "\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    num_leaves=60,\n",
    "    min_child_samples=40,\n",
    "    reg_alpha=3,\n",
    "    reg_lambda=3,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    # (\"pca\", pca),\n",
    "    # (\"resample\", resampler),\n",
    "    (\"model\", lgbm)\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "print(\"Training pipeline ...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate\n",
    "# -------------------------------\n",
    "preds_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, preds_proba)\n",
    "print(f\"Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Optional: classification report at threshold 0.5\n",
    "preds = (preds_proba >= 0.5).astype(int)\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "# ROC curve save\n",
    "fpr, tpr, thr = roc_curve(y_test, preds_proba)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc:.4f}\")\n",
    "plt.plot([0,1],[0,1], linestyle='--', alpha=0.6)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"roc_curve.png\")\n",
    "plt.close()\n",
    "\n",
    "# -------------------------------\n",
    "# Save artifacts\n",
    "# -------------------------------\n",
    "joblib.dump(pipeline, MODEL_OUTPUT)\n",
    "print(f\"Saved pipeline to {MODEL_OUTPUT}\")\n",
    "\n",
    "# Save preprocessor separately if needed\n",
    "joblib.dump(preprocessor, PREPROCESSOR_OUTPUT)\n",
    "print(f\"Saved preprocessor to {PREPROCESSOR_OUTPUT}\")\n",
    "\n",
    "# -------------------------------\n",
    "# SHAP explainability\n",
    "# -------------------------------\n",
    "print(\"Computing SHAP values (may take some time)...\")\n",
    "\n",
    "# We need to extract the fitted LGBM and the transformed training matrix for SHAP\n",
    "fitted_model = pipeline.named_steps['model']\n",
    "preproc_step = pipeline.named_steps['preprocess']\n",
    "\n",
    "# Transform a subset for speed and stability\n",
    "X_train_trans = preproc_step.transform(X_train)\n",
    "\n",
    "# Use TreeExplainer\n",
    "explainer = shap.TreeExplainer(fitted_model)\n",
    "shap_values = explainer.shap_values(X_train_trans,)\n",
    "\n",
    "# Create feature names for transformed matrix\n",
    "try:\n",
    "    transformed_feature_names = preproc_step.get_feature_names_out()\n",
    "except Exception:\n",
    "    # Fallback: build names\n",
    "    num_names = num_features\n",
    "    cat_names = list(preproc_step.named_transformers_[\"cat\"].get_feature_names_out(cat_features)) if len(cat_features)>0 else []\n",
    "    transformed_feature_names = np.array(num_names + cat_names)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(10,8))\n",
    "shap.summary_plot(shap_values, X_train_trans, feature_names=transformed_feature_names, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(SHAP_PLOT_OUTPUT)\n",
    "plt.close()\n",
    "print(f\"Saved SHAP summary to {SHAP_PLOT_OUTPUT}\")\n",
    "\n",
    "# Optional: aggregate SHAP to original features if one-hot expanded\n",
    "# This part aggregates OHE names back to base features (if OHE used)\n",
    "try:\n",
    "    tf_names = list(transformed_feature_names)\n",
    "    shap_vals_mean = np.mean(np.abs(shap_values), axis=0)\n",
    "    agg = {}\n",
    "    for nm, val in zip(tf_names, shap_vals_mean):\n",
    "        if '__' in nm:\n",
    "            # sklearn get_feature_names_out uses format: \"cat__<col>_<level>\" or similar\n",
    "            base = nm.split(\"__\")[1] if '__' in nm else nm\n",
    "            # sometimes OneHotEncoder uses 'col_level'\n",
    "            if '_' in base and base.split('_')[0] in cat_features:\n",
    "                base = base.split('_')[0]\n",
    "        else:\n",
    "            base = nm\n",
    "        agg[base] = agg.get(base, 0) + val\n",
    "    agg_df = pd.DataFrame([ (k,v) for k,v in agg.items() ], columns=[\"feature\",\"shap_abs_mean\"]).sort_values(by='shap_abs_mean', ascending=False)\n",
    "    agg_df.to_csv('shap_aggregated.csv', index=False)\n",
    "    print('Saved aggregated SHAP importances to shap_aggregated.csv')\n",
    "except Exception as e:\n",
    "    print('Failed to aggregate SHAP:', e)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ca7146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CREDIT DEFAULT PREDICTION - COMPLETE PIPELINE\n",
      "Based on: arXiv:2408.03497\n",
      "======================================================================\n",
      "======================================================================\n",
      "CREDIT DEFAULT PREPROCESSING - RESEARCH PAPER METHODOLOGY\n",
      "======================================================================\n",
      "\n",
      "[1] Initial Dataset:\n",
      "   Samples: 89,999\n",
      "   Features: 67\n",
      "   Default rate: 5.10%\n",
      "   Imbalance ratio: 18.6:1\n",
      "\n",
      "[2] Dropped 0 ID/noise columns\n",
      "\n",
      "[3] Handling missing values...\n",
      "\n",
      "[4] Capping outliers...\n",
      "\n",
      "[5] Engineering features...\n",
      "   Created 25 new features\n",
      "\n",
      "[6] Encoding categorical variables...\n",
      "\n",
      "[7] Removing redundant features...\n",
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "   Final features: 85\n",
      "   Ready for modeling\n",
      "\n",
      "======================================================================\n",
      "DATA SPLIT\n",
      "======================================================================\n",
      "\n",
      "Training set: 71,999 samples (5.10% default)\n",
      "Test set:     18,000 samples (5.11% default)\n",
      "\n",
      "======================================================================\n",
      "XGBOOST MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Class imbalance handling:\n",
      "   scale_pos_weight: 18.59\n",
      "\n",
      "Model hyperparameters:\n",
      "   n_estimators: 500\n",
      "   max_depth: 7\n",
      "   learning_rate: 0.03\n",
      "   subsample: 0.8\n",
      "   colsample_bytree: 0.8\n",
      "   colsample_bylevel: 0.8\n",
      "   min_child_weight: 3\n",
      "   gamma: 0.1\n",
      "   reg_alpha: 0.1\n",
      "   reg_lambda: 1.0\n",
      "   early_stopping_rounds: 50\n",
      "   scale_pos_weight: 18.59156462585034\n",
      "   objective: binary:logistic\n",
      "   eval_metric: auc\n",
      "\n",
      "[1] Training XGBoost...\n",
      "   Best iteration: 31\n",
      "\n",
      "[2] Model Evaluation:\n",
      "   Training AUC:   0.8619\n",
      "   Test AUC:       0.8044\n",
      "   Overfitting:    0.0576\n",
      "\n",
      "   Competition Score: ‚úÖ VERY GOOD (23-24/25 points)\n",
      "\n",
      "[3] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default      0.978     0.798     0.879     17081\n",
      "     Default      0.152     0.672     0.248       919\n",
      "\n",
      "    accuracy                          0.792     18000\n",
      "   macro avg      0.565     0.735     0.563     18000\n",
      "weighted avg      0.936     0.792     0.847     18000\n",
      "\n",
      "[4] Confusion Matrix:\n",
      "   True Negatives:  13,630\n",
      "   False Positives: 3,451\n",
      "   False Negatives: 301\n",
      "   True Positives:  618\n",
      "\n",
      "[5] Business Metrics:\n",
      "   Sensitivity (Recall): 67.25% - Caught 67% of defaults\n",
      "   Specificity:          79.80% - Correctly identified 80% of non-defaults\n",
      "   Precision:            15.19% - 15% of predicted defaults were correct\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Performing 5-fold stratified cross-validation...\n",
      "\n",
      "CV AUC Scores by Fold:\n",
      "   Fold 1: 0.7794\n",
      "   Fold 2: 0.7933\n",
      "   Fold 3: 0.7764\n",
      "   Fold 4: 0.7715\n",
      "   Fold 5: 0.7781\n",
      "\n",
      "   Mean CV AUC: 0.7797\n",
      "   Std Dev:     0.0073\n",
      "   95% CI:      [0.7654, 0.7940]\n",
      "\n",
      "======================================================================\n",
      "TOP 20 MOST IMPORTANT FEATURES\n",
      "======================================================================\n",
      "\n",
      "                    feature  importance\n",
      "    monthly_free_cash_flow    0.100886\n",
      "   credit_score_normalized    0.054013\n",
      "              credit_score    0.045222\n",
      "             annual_income    0.041084\n",
      "                       age    0.032660\n",
      "        debt_service_ratio    0.028432\n",
      "         total_debt_burden    0.021035\n",
      "      debt_to_income_ratio    0.019557\n",
      "          poor_credit_flag    0.018409\n",
      "          high_utilization    0.015095\n",
      "loan_purpose_Home Purchase    0.014324\n",
      "   payment_to_income_ratio    0.013716\n",
      "        loan_type_Personal    0.013705\n",
      "      very_low_utilization    0.013703\n",
      "     excellent_credit_flag    0.012998\n",
      "   preferred_contact_Phone    0.011686\n",
      "           has_delinquency    0.011603\n",
      "       num_credit_accounts    0.011443\n",
      "        num_public_records    0.011273\n",
      "        credit_utilization    0.010732\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Test AUC:        0.8044\n",
      "‚úÖ Mean CV AUC:     0.7797 (¬±0.0073)\n",
      "‚úÖ Total Features:  85\n",
      "‚úÖ Training Time:   ~31 iterations\n",
      "\n",
      "üéØ TARGET ACHIEVED! Ready for competition submission.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Credit Default Prediction Pipeline\n",
    "Based on: XGBoost for Credit Default Prediction - arXiv:2408.03497\n",
    "\n",
    "Target: Achieve >80% AUC score for competition\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== PHASE 1: DATA PREPROCESSING ====================\n",
    "\n",
    "def preprocess_credit_data(df, target='default'):\n",
    "    \"\"\"\n",
    "    Preprocessing based on research paper methodology\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CREDIT DEFAULT PREPROCESSING - RESEARCH PAPER METHODOLOGY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Separate target\n",
    "    y = df[target].copy()\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    print(f\"\\n[1] Initial Dataset:\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Default rate: {y.mean():.2%}\")\n",
    "    print(f\"   Imbalance ratio: {(1-y.mean())/y.mean():.1f}:1\")\n",
    "    \n",
    "    # Drop identifiers and noise\n",
    "    drop_cols = ['customer_id', 'application_id', 'loan_officer_id', \n",
    "                 'random_noise_1', 'recent_inquiry_count', 'oldest_credit_line_age']\n",
    "    X = X.drop([col for col in drop_cols if col in X.columns], axis=1)\n",
    "    print(f\"\\n[2] Dropped {len([c for c in drop_cols if c in df.columns])} ID/noise columns\")\n",
    "    \n",
    "    # Handle missing values with flags\n",
    "    print(f\"\\n[3] Handling missing values...\")\n",
    "    missing_cols = X.columns[X.isnull().any()].tolist()\n",
    "    \n",
    "    for col in missing_cols:\n",
    "        if X[col].dtype in ['float64', 'int64']:\n",
    "            X[f'{col}_missing'] = X[col].isnull().astype(int)\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "            print(f\"   - {col}: {X[f'{col}_missing'].sum()} missing ‚Üí filled with median\")\n",
    "    \n",
    "    # Cap outliers at 1st and 99th percentiles\n",
    "    print(f\"\\n[4] Capping outliers...\")\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    outlier_features = ['loan_amount', 'annual_income', 'total_credit_limit',\n",
    "                        'revolving_balance', 'annual_debt_payment']\n",
    "    \n",
    "    for col in outlier_features:\n",
    "        if col in X.columns:\n",
    "            lower, upper = X[col].quantile([0.01, 0.99])\n",
    "            X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Feature engineering based on paper\n",
    "    print(f\"\\n[5] Engineering features...\")\n",
    "    \n",
    "    # Debt burden indicators\n",
    "    if all(c in X.columns for c in ['debt_to_income_ratio', 'payment_to_income_ratio']):\n",
    "        X['total_debt_burden'] = X['debt_to_income_ratio'] + X['payment_to_income_ratio']\n",
    "        X['debt_stress_indicator'] = (X['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "    \n",
    "    # Credit risk score (paper's approach)\n",
    "    if 'credit_score' in X.columns:\n",
    "        X['credit_score_normalized'] = (X['credit_score'] - 300) / (850 - 300)\n",
    "        X['poor_credit_flag'] = (X['credit_score'] < 650).astype(int)\n",
    "        X['excellent_credit_flag'] = (X['credit_score'] > 750).astype(int)\n",
    "    \n",
    "    # Delinquency indicators\n",
    "    if 'num_delinquencies_2yrs' in X.columns:\n",
    "        X['has_delinquency'] = (X['num_delinquencies_2yrs'] > 0).astype(int)\n",
    "        X['multiple_delinquencies'] = (X['num_delinquencies_2yrs'] > 1).astype(int)\n",
    "    \n",
    "    # Utilization features\n",
    "    if 'credit_utilization' in X.columns:\n",
    "        X['high_utilization'] = (X['credit_utilization'] > 0.75).astype(int)\n",
    "        X['very_low_utilization'] = (X['credit_utilization'] < 0.1).astype(int)\n",
    "    \n",
    "    # Income adequacy\n",
    "    if 'annual_income' in X.columns and 'loan_amount' in X.columns:\n",
    "        X['loan_to_income_ratio'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "        X['income_adequacy'] = (X['annual_income'] / 12) / (X['loan_amount'] / 60 + 1)\n",
    "    \n",
    "    # Employment stability\n",
    "    if 'employment_length' in X.columns:\n",
    "        X['stable_employment'] = (X['employment_length'] >= 3).astype(int)\n",
    "        X['employment_years_squared'] = X['employment_length'] ** 2\n",
    "    \n",
    "    # Account age features\n",
    "    if 'oldest_account_age_months' in X.columns:\n",
    "        X['account_age_years'] = X['oldest_account_age_months'] / 12\n",
    "        X['thin_credit_file'] = (X['oldest_account_age_months'] < 24).astype(int)\n",
    "    \n",
    "    # Inquiry intensity\n",
    "    if 'num_inquiries_6mo' in X.columns:\n",
    "        X['credit_shopping'] = (X['num_inquiries_6mo'] > 2).astype(int)\n",
    "        X['excessive_inquiries'] = (X['num_inquiries_6mo'] > 4).astype(int)\n",
    "    \n",
    "    # Interest rate indicators (risk pricing)\n",
    "    if 'interest_rate' in X.columns:\n",
    "        X['subprime_rate'] = (X['interest_rate'] > 15).astype(int)\n",
    "        X['prime_rate'] = (X['interest_rate'] < 8).astype(int)\n",
    "    \n",
    "    print(f\"   Created {X.shape[1] - len(df.columns) + len(drop_cols) + 1} new features\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(f\"\\n[6] Encoding categorical variables...\")\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Low cardinality: one-hot encoding\n",
    "    low_card = [col for col in categorical_cols if X[col].nunique() < 10]\n",
    "    if low_card:\n",
    "        X = pd.get_dummies(X, columns=low_card, drop_first=True, dtype=int)\n",
    "        print(f\"   One-hot encoded: {low_card}\")\n",
    "    \n",
    "    # High cardinality: frequency encoding\n",
    "    high_card = [col for col in categorical_cols if X[col].nunique() >= 10]\n",
    "    for col in high_card:\n",
    "        freq_map = X[col].value_counts(normalize=True).to_dict()\n",
    "        X[f'{col}_frequency'] = X[col].map(freq_map)\n",
    "        X = X.drop(col, axis=1)\n",
    "        print(f\"   Frequency encoded: {col}\")\n",
    "    \n",
    "    # Remove redundant features\n",
    "    print(f\"\\n[7] Removing redundant features...\")\n",
    "    redundant = ['monthly_income']  # Redundant with annual_income\n",
    "    X = X.drop([col for col in redundant if col in X.columns], axis=1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   Final features: {X.shape[1]}\")\n",
    "    print(f\"   Ready for modeling\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# ==================== PHASE 2: MODEL TRAINING ====================\n",
    "\n",
    "def train_xgboost_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    XGBoost training based on paper's optimal hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"XGBOOST MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalance\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f\"\\nClass imbalance handling:\")\n",
    "    print(f\"   scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    # Paper's recommended hyperparameters (optimized for credit default)\n",
    "    params = {\n",
    "        'n_estimators': 500,           # More trees for better performance\n",
    "        'max_depth': 7,                # Deeper trees for complex patterns\n",
    "        'learning_rate': 0.03,         # Lower LR with more trees\n",
    "        'subsample': 0.8,              # Row sampling to prevent overfitting\n",
    "        'colsample_bytree': 0.8,       # Column sampling\n",
    "        'colsample_bylevel': 0.8,      # Column sampling per level\n",
    "        'min_child_weight': 3,         # Minimum sum of weights\n",
    "        'gamma': 0.1,                  # Minimum loss reduction\n",
    "        'reg_alpha': 0.1,              # L1 regularization\n",
    "        'reg_lambda': 1.0,             # L2 regularization\n",
    "        'early_stopping_rounds' : 50,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist'          # Faster training\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nModel hyperparameters:\")\n",
    "    for key, value in params.items():\n",
    "        if key not in ['n_jobs', 'random_state', 'tree_method']:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\n[1] Training XGBoost...\")\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Use early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    print(f\"   Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n[2] Model Evaluation:\")\n",
    "    \n",
    "    # Training performance\n",
    "    y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Test performance\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"   Training AUC:   {train_auc:.4f}\")\n",
    "    print(f\"   Test AUC:       {test_auc:.4f}\")\n",
    "    print(f\"   Overfitting:    {train_auc - test_auc:.4f}\")\n",
    "    \n",
    "    # Competition scoring\n",
    "    if test_auc >= 0.85:\n",
    "        grade = \"üèÜ EXCELLENT (25/25 points)\"\n",
    "    elif test_auc >= 0.80:\n",
    "        grade = \"‚úÖ VERY GOOD (23-24/25 points)\"\n",
    "    elif test_auc >= 0.75:\n",
    "        grade = \"‚úÖ GOOD (20-22/25 points)\"\n",
    "    elif test_auc >= 0.60:\n",
    "        grade = \"‚ö†Ô∏è  PASSING (15-19/25 points)\"\n",
    "    else:\n",
    "        grade = \"‚ùå DISQUALIFIED (<15 points)\"\n",
    "    \n",
    "    print(f\"\\n   Competition Score: {grade}\")\n",
    "    \n",
    "    # Classification metrics\n",
    "    y_test_class = model.predict(X_test)\n",
    "    print(f\"\\n[3] Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_class, \n",
    "                                target_names=['No Default', 'Default'],\n",
    "                                digits=3))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"[4] Confusion Matrix:\")\n",
    "    print(f\"   True Negatives:  {tn:,}\")\n",
    "    print(f\"   False Positives: {fp:,}\")\n",
    "    print(f\"   False Negatives: {fn:,}\")\n",
    "    print(f\"   True Positives:  {tp:,}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n[5] Business Metrics:\")\n",
    "    print(f\"   Sensitivity (Recall): {sensitivity:.2%} - Caught {sensitivity:.0%} of defaults\")\n",
    "    print(f\"   Specificity:          {specificity:.2%} - Correctly identified {specificity:.0%} of non-defaults\")\n",
    "    print(f\"   Precision:            {precision:.2%} - {precision:.0%} of predicted defaults were correct\")\n",
    "    \n",
    "    return model, test_auc\n",
    "\n",
    "# ==================== PHASE 3: CROSS-VALIDATION ====================\n",
    "\n",
    "def cross_validate_model(X, y, params):\n",
    "    \"\"\"\n",
    "    5-fold stratified cross-validation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    print(f\"\\nPerforming 5-fold stratified cross-validation...\")\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X, y, \n",
    "        cv=skf, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCV AUC Scores by Fold:\")\n",
    "    for i, score in enumerate(cv_scores, 1):\n",
    "        print(f\"   Fold {i}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Mean CV AUC: {cv_scores.mean():.4f}\")\n",
    "    print(f\"   Std Dev:     {cv_scores.std():.4f}\")\n",
    "    print(f\"   95% CI:      [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, \"\n",
    "          f\"{cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "def main_pipeline(df):\n",
    "    \"\"\"\n",
    "    Complete pipeline execution\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREDIT DEFAULT PREDICTION - COMPLETE PIPELINE\")\n",
    "    print(\"Based on: arXiv:2408.03497\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Preprocess\n",
    "    X, y = preprocess_credit_data(df)\n",
    "    \n",
    "    # Split data (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        stratify=y, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTraining set: {X_train.shape[0]:,} samples ({y_train.mean():.2%} default)\")\n",
    "    print(f\"Test set:     {X_test.shape[0]:,} samples ({y_test.mean():.2%} default)\")\n",
    "    \n",
    "    # Train model\n",
    "    model, test_auc = train_xgboost_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    params = {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 7,\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    cv_scores = cross_validate_model(X_train, y_train, params)\n",
    "    \n",
    "    # Feature importance\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TOP 20 MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\", feature_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚úÖ Test AUC:        {test_auc:.4f}\")\n",
    "    print(f\"‚úÖ Mean CV AUC:     {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    print(f\"‚úÖ Total Features:  {X_train.shape[1]}\")\n",
    "    print(f\"‚úÖ Training Time:   ~{model.best_iteration} iterations\")\n",
    "    \n",
    "    if test_auc >= 0.80:\n",
    "        print(f\"\\nüéØ TARGET ACHIEVED! Ready for competition submission.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  AUC below 80%. Consider:\")\n",
    "        print(f\"   - Hyperparameter tuning\")\n",
    "        print(f\"   - Feature engineering\")\n",
    "        print(f\"   - Ensemble methods\")\n",
    "    \n",
    "    return model, feature_importance\n",
    "\n",
    "# ==================== USAGE ====================\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/final.csv')\n",
    "\n",
    "# Run complete pipeline\n",
    "model, feature_importance = main_pipeline(data)\n",
    "\n",
    "# For new predictions:\n",
    "# predictions = model.predict_proba(X_new)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2097b2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function abspath at 0x1053014e0>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    " # Prints the parent directory of the current file's directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "545c7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CATBOOST CREDIT DEFAULT PREDICTION\n",
      "======================================================================\n",
      "======================================================================\n",
      "PREPROCESSING FOR CATBOOST\n",
      "======================================================================\n",
      "\n",
      "[1] Initial Dataset:\n",
      "   Samples: 89,999\n",
      "   Features: 67\n",
      "   Default rate: 5.10%\n",
      "\n",
      "[2] Handling missing values...\n",
      "\n",
      "[3] Capping outliers...\n",
      "\n",
      "[4] Engineering features...\n",
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "   Final features: 79\n",
      "   Categorical features: 0\n",
      "   Categorical columns: []\n",
      "\n",
      "======================================================================\n",
      "DATA SPLIT\n",
      "======================================================================\n",
      "\n",
      "Training set: 71,999 samples (5.10% default)\n",
      "Test set:     18,000 samples (5.11% default)\n",
      "\n",
      "======================================================================\n",
      "CATBOOST MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Class imbalance: 18.59:1\n",
      "\n",
      "Model hyperparameters:\n",
      "   iterations: 1000\n",
      "   learning_rate: 0.03\n",
      "   depth: 7\n",
      "   l2_leaf_reg: 3\n",
      "   min_data_in_leaf: 20\n",
      "   max_bin: 254\n",
      "   subsample: 0.8\n",
      "   colsample_bylevel: 0.8\n",
      "   random_strength: 1\n",
      "   bagging_temperature: 1\n",
      "   auto_class_weights: Balanced\n",
      "   eval_metric: AUC\n",
      "   early_stopping_rounds: 50\n",
      "\n",
      "[1] Training CatBoost...\n",
      "0:\ttest: 0.7735101\tbest: 0.7735101 (0)\ttotal: 78.2ms\tremaining: 1m 18s\n",
      "100:\ttest: 0.8063366\tbest: 0.8063366 (100)\ttotal: 2.21s\tremaining: 19.7s\n",
      "200:\ttest: 0.8092306\tbest: 0.8094255 (186)\ttotal: 3.27s\tremaining: 13s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8094842732\n",
      "bestIteration = 208\n",
      "\n",
      "Shrink model to first 209 iterations.\n",
      "\n",
      "   Best iteration: 208\n",
      "   Best validation AUC: 0.8095\n",
      "\n",
      "[2] Model Evaluation:\n",
      "   Training AUC:   0.8438\n",
      "   Test AUC:       0.8095\n",
      "   Overfitting:    0.0344\n",
      "\n",
      "   Competition Score: ‚úÖ VERY GOOD (23-24/25 points)\n",
      "\n",
      "[3] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default      0.980     0.767     0.860     17081\n",
      "     Default      0.140     0.705     0.234       919\n",
      "\n",
      "    accuracy                          0.764     18000\n",
      "   macro avg      0.560     0.736     0.547     18000\n",
      "weighted avg      0.937     0.764     0.828     18000\n",
      "\n",
      "[4] Confusion Matrix:\n",
      "   True Negatives:  13,100\n",
      "   False Positives: 3,981\n",
      "   False Negatives: 271\n",
      "   True Positives:  648\n",
      "\n",
      "[5] Business Metrics:\n",
      "   Sensitivity: 70.51% - Caught 71% of defaults\n",
      "   Specificity: 76.69% - Correctly identified 77% of non-defaults\n",
      "   Precision:   14.00% - 14% of predicted defaults were correct\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Performing 5-fold stratified cross-validation...\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "Parameter loss_function should be specified for cross-validation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 419\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# ==================== USAGE EXAMPLES ====================\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# EXAMPLE 1: Basic Usage\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# ----------------------\u001b[39;00m\n\u001b[1;32m    418\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/final.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 419\u001b[0m model, importance, cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mmain_catboost_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# predictions = model.predict_proba(X_new)[:, 1]\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# model.fit(X_train, y_train, eval_set=(X_test, y_test))\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# print(f\"AUC: {roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]):.4f}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 337\u001b[0m, in \u001b[0;36mmain_catboost_pipeline\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    334\u001b[0m model, test_auc \u001b[38;5;241m=\u001b[39m train_catboost(X_train, y_train, X_test, y_test, cat_features)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Cross-validation\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m cv_auc, cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcatboost_cross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Feature importance\u001b[39;00m\n\u001b[1;32m    340\u001b[0m importance_df \u001b[38;5;241m=\u001b[39m plot_feature_importance(model, X_train, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 259\u001b[0m, in \u001b[0;36mcatboost_cross_validate\u001b[0;34m(X, y, cat_features)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Perform 5-fold CV\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPerforming 5-fold stratified cross-validation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstratified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_random_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Extract AUC scores\u001b[39;00m\n\u001b[1;32m    271\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cv_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-AUC-mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/opt/miniconda3/envs/api-visn/lib/python3.12/site-packages/catboost/core.py:6859\u001b[0m, in \u001b[0;36mcv\u001b[0;34m(pool, params, dtrain, iterations, num_boost_round, fold_count, nfold, inverted, partition_random_seed, seed, shuffle, logging_level, stratified, as_pandas, metric_period, verbose, verbose_eval, plot, plot_file, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, metric_update_interval, folds, type, return_models, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   6855\u001b[0m metric_period, verbose, logging_level \u001b[38;5;241m=\u001b[39m _process_verbose(\n\u001b[1;32m   6856\u001b[0m     metric_period, verbose, logging_level, verbose_eval)\n\u001b[1;32m   6858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m-> 6859\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter loss_function should be specified for cross-validation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [fold_count, nfold]) \u001b[38;5;129;01mand\u001b[39;00m folds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6862\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CatBoostError(\n\u001b[1;32m   6863\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif folds is not None, then all of fold_count, shuffle, partition_random_seed, inverted are None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6864\u001b[0m     )\n",
      "\u001b[0;31mCatBoostError\u001b[0m: Parameter loss_function should be specified for cross-validation"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CatBoost Credit Default Prediction Pipeline\n",
    "Optimized for 5.1% imbalanced dataset\n",
    "Target: 80%+ AUC score\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== PREPROCESSING FOR CATBOOST ====================\n",
    "\n",
    "def preprocess_for_catboost(df, target='default'):\n",
    "    \"\"\"\n",
    "    CatBoost-specific preprocessing\n",
    "    Advantage: CatBoost handles categorical features natively!\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREPROCESSING FOR CATBOOST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Separate target\n",
    "    y = df[target].copy()\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    print(f\"\\n[1] Initial Dataset:\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Default rate: {y.mean():.2%}\")\n",
    "    \n",
    "    # Drop identifiers and noise\n",
    "    drop_cols = ['customer_id', 'application_id', 'loan_officer_id', \n",
    "                 'random_noise_1', 'recent_inquiry_count', 'oldest_credit_line_age']\n",
    "    X = X.drop([col for col in drop_cols if col in X.columns], axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\n[2] Handling missing values...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                X[f'{col}_missing'] = X[col].isnull().astype(int)\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Cap outliers\n",
    "    print(f\"\\n[3] Capping outliers...\")\n",
    "    outlier_features = ['loan_amount', 'annual_income', 'total_credit_limit',\n",
    "                        'revolving_balance', 'annual_debt_payment']\n",
    "    for col in outlier_features:\n",
    "        if col in X.columns:\n",
    "            lower, upper = X[col].quantile([0.01, 0.99])\n",
    "            X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(f\"\\n[4] Engineering features...\")\n",
    "    \n",
    "    # Debt features\n",
    "    if all(c in X.columns for c in ['debt_to_income_ratio', 'payment_to_income_ratio']):\n",
    "        X['total_debt_burden'] = X['debt_to_income_ratio'] + X['payment_to_income_ratio']\n",
    "        X['debt_stress'] = (X['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "    \n",
    "    # Credit score features\n",
    "    if 'credit_score' in X.columns:\n",
    "        X['credit_score_norm'] = (X['credit_score'] - 300) / 550\n",
    "        X['poor_credit'] = (X['credit_score'] < 650).astype(int)\n",
    "        X['excellent_credit'] = (X['credit_score'] > 750).astype(int)\n",
    "    \n",
    "    # Delinquency features\n",
    "    if 'num_delinquencies_2yrs' in X.columns:\n",
    "        X['has_delinquency'] = (X['num_delinquencies_2yrs'] > 0).astype(int)\n",
    "    \n",
    "    # Utilization\n",
    "    if 'credit_utilization' in X.columns:\n",
    "        X['high_utilization'] = (X['credit_utilization'] > 0.75).astype(int)\n",
    "    \n",
    "    # Income adequacy\n",
    "    if 'annual_income' in X.columns and 'loan_amount' in X.columns:\n",
    "        X['loan_to_income'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "    \n",
    "    # Employment\n",
    "    if 'employment_length' in X.columns:\n",
    "        X['stable_employment'] = (X['employment_length'] >= 3).astype(int)\n",
    "    \n",
    "    # Account age\n",
    "    if 'oldest_account_age_months' in X.columns:\n",
    "        X['thin_credit_file'] = (X['oldest_account_age_months'] < 24).astype(int)\n",
    "    \n",
    "    # Inquiries\n",
    "    if 'num_inquiries_6mo' in X.columns:\n",
    "        X['excessive_inquiries'] = (X['num_inquiries_6mo'] > 4).astype(int)\n",
    "    \n",
    "    # Interest rate\n",
    "    if 'interest_rate' in X.columns:\n",
    "        X['subprime_rate'] = (X['interest_rate'] > 15).astype(int)\n",
    "    \n",
    "    # Identify categorical features for CatBoost\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # CatBoost handles categoricals, so DON'T one-hot encode!\n",
    "    # Just convert to 'category' dtype\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].astype('category')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   Final features: {X.shape[1]}\")\n",
    "    print(f\"   Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"   Categorical columns: {categorical_features}\")\n",
    "    \n",
    "    return X, y, categorical_features\n",
    "\n",
    "# ==================== CATBOOST TRAINING ====================\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test, cat_features=None):\n",
    "    \"\"\"\n",
    "    Train CatBoost with optimal hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATBOOST MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f\"\\nClass imbalance: {scale_pos_weight:.2f}:1\")\n",
    "    \n",
    "    # CatBoost parameters (optimized for credit default)\n",
    "    params = {\n",
    "        'iterations': 1000,              # Max iterations\n",
    "        'learning_rate': 0.03,           # Learning rate\n",
    "        'depth': 7,                      # Tree depth\n",
    "        'l2_leaf_reg': 3,                # L2 regularization\n",
    "        'min_data_in_leaf': 20,          # Min samples per leaf\n",
    "        'max_bin': 254,                  # Max bins for numerical features\n",
    "        'subsample': 0.8,                # Row sampling\n",
    "        'colsample_bylevel': 0.8,        # Column sampling per level\n",
    "        'random_strength': 1,            # Randomness for scoring splits\n",
    "        'bagging_temperature': 1,        # Bayesian bootstrap temperature\n",
    "        'auto_class_weights': 'Balanced', # Handle imbalance automatically\n",
    "        'eval_metric': 'AUC',            # Evaluation metric\n",
    "        'early_stopping_rounds': 50,     # Early stopping\n",
    "        'random_seed': 42,\n",
    "        'verbose': 100,                  # Print every 100 iterations\n",
    "        'task_type': 'CPU',              # Use GPU if available: 'GPU'\n",
    "        'thread_count': -1               # Use all CPU cores\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nModel hyperparameters:\")\n",
    "    for key, value in params.items():\n",
    "        if key not in ['verbose', 'task_type', 'thread_count', 'random_seed']:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Create CatBoost model\n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Train with validation set\n",
    "    print(f\"\\n[1] Training CatBoost...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_features,      # Specify categorical features\n",
    "        eval_set=(X_test, y_test),\n",
    "        use_best_model=True,             # Use best iteration\n",
    "        plot=False                       # Set True for training visualization\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   Best iteration: {model.best_iteration_}\")\n",
    "    print(f\"   Best validation AUC: {model.best_score_['validation']['AUC']:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n[2] Model Evaluation:\")\n",
    "    \n",
    "    # Training performance\n",
    "    y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Test performance\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"   Training AUC:   {train_auc:.4f}\")\n",
    "    print(f\"   Test AUC:       {test_auc:.4f}\")\n",
    "    print(f\"   Overfitting:    {train_auc - test_auc:.4f}\")\n",
    "    \n",
    "    # Competition scoring\n",
    "    if test_auc >= 0.85:\n",
    "        grade = \"üèÜ EXCELLENT (25/25 points)\"\n",
    "    elif test_auc >= 0.80:\n",
    "        grade = \"‚úÖ VERY GOOD (23-24/25 points)\"\n",
    "    elif test_auc >= 0.75:\n",
    "        grade = \"‚úÖ GOOD (20-22/25 points)\"\n",
    "    elif test_auc >= 0.60:\n",
    "        grade = \"‚ö†Ô∏è  PASSING (15-19/25 points)\"\n",
    "    else:\n",
    "        grade = \"‚ùå DISQUALIFIED (<15 points)\"\n",
    "    \n",
    "    print(f\"\\n   Competition Score: {grade}\")\n",
    "    \n",
    "    # Classification report\n",
    "    y_test_class = model.predict(X_test)\n",
    "    print(f\"\\n[3] Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_class,\n",
    "                                target_names=['No Default', 'Default'],\n",
    "                                digits=3))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"[4] Confusion Matrix:\")\n",
    "    print(f\"   True Negatives:  {tn:,}\")\n",
    "    print(f\"   False Positives: {fp:,}\")\n",
    "    print(f\"   False Negatives: {fn:,}\")\n",
    "    print(f\"   True Positives:  {tp:,}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n[5] Business Metrics:\")\n",
    "    print(f\"   Sensitivity: {sensitivity:.2%} - Caught {sensitivity:.0%} of defaults\")\n",
    "    print(f\"   Specificity: {specificity:.2%} - Correctly identified {specificity:.0%} of non-defaults\")\n",
    "    print(f\"   Precision:   {precision:.2%} - {precision:.0%} of predicted defaults were correct\")\n",
    "    \n",
    "    return model, test_auc\n",
    "\n",
    "# ==================== CROSS-VALIDATION ====================\n",
    "\n",
    "def catboost_cross_validate(X, y, cat_features=None):\n",
    "    \"\"\"\n",
    "    5-fold stratified cross-validation with CatBoost\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create Pool object (CatBoost's data structure)\n",
    "    pool = Pool(\n",
    "        data=X,\n",
    "        label=y,\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "    \n",
    "    # CV parameters\n",
    "    params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'auto_class_weights': 'Balanced',\n",
    "        'eval_metric': 'AUC',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    # Perform 5-fold CV\n",
    "    print(f\"\\nPerforming 5-fold stratified cross-validation...\")\n",
    "    cv_results = cv(\n",
    "        pool=pool,\n",
    "        params=params,\n",
    "        fold_count=5,\n",
    "        stratified=True,\n",
    "        partition_random_seed=42,\n",
    "        shuffle=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Extract AUC scores\n",
    "    cv_scores = cv_results['test-AUC-mean'].values\n",
    "    best_iteration = cv_results['test-AUC-mean'].idxmax()\n",
    "    \n",
    "    print(f\"\\nCV Results:\")\n",
    "    print(f\"   Best iteration: {best_iteration}\")\n",
    "    print(f\"   Best CV AUC:    {cv_scores[best_iteration]:.4f}\")\n",
    "    print(f\"   Final CV AUC:   {cv_scores[-1]:.4f}\")\n",
    "    print(f\"   Std Dev:        {cv_results['test-AUC-std'].values[best_iteration]:.4f}\")\n",
    "    \n",
    "    return cv_scores[best_iteration], cv_results\n",
    "\n",
    "# ==================== FEATURE IMPORTANCE ====================\n",
    "\n",
    "def plot_feature_importance(model, X, top_n=20):\n",
    "    \"\"\"\n",
    "    Display feature importance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP {top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{importance_df.head(top_n).to_string(index=False)}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def main_catboost_pipeline(df):\n",
    "    \"\"\"\n",
    "    Complete CatBoost pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATBOOST CREDIT DEFAULT PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Preprocess\n",
    "    X, y, cat_features = preprocess_for_catboost(df)\n",
    "    \n",
    "    # Split data (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTraining set: {X_train.shape[0]:,} samples ({y_train.mean():.2%} default)\")\n",
    "    print(f\"Test set:     {X_test.shape[0]:,} samples ({y_test.mean():.2%} default)\")\n",
    "    \n",
    "    # Train model\n",
    "    model, test_auc = train_catboost(X_train, y_train, X_test, y_test, cat_features)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_auc, cv_results = catboost_cross_validate(X_train, y_train, cat_features)\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = plot_feature_importance(model, X_train, top_n=20)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚úÖ Test AUC:           {test_auc:.4f}\")\n",
    "    print(f\"‚úÖ Cross-Val AUC:      {cv_auc:.4f}\")\n",
    "    print(f\"‚úÖ Best Iteration:     {model.best_iteration_}\")\n",
    "    print(f\"‚úÖ Total Features:     {X_train.shape[1]}\")\n",
    "    print(f\"‚úÖ Categorical Feats:  {len(cat_features)}\")\n",
    "    \n",
    "    if test_auc >= 0.80:\n",
    "        print(f\"\\nüéØ TARGET ACHIEVED! Ready for competition.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  AUC below 80%. Consider hyperparameter tuning.\")\n",
    "    \n",
    "    return model, importance_df, cv_results\n",
    "\n",
    "# ==================== HYPERPARAMETER TUNING ====================\n",
    "\n",
    "def tune_catboost(X_train, y_train, cat_features=None):\n",
    "    \"\"\"\n",
    "    Grid search for optimal hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    \n",
    "    # Parameter grid\n",
    "    param_distributions = {\n",
    "        'depth': [5, 6, 7, 8, 9],\n",
    "        'learning_rate': [0.01, 0.02, 0.03, 0.05],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7],\n",
    "        'min_data_in_leaf': [10, 20, 30],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bylevel': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Base model\n",
    "    base_model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        auto_class_weights='Balanced',\n",
    "        eval_metric='AUC',\n",
    "        early_stopping_rounds=50,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Randomized search\n",
    "    print(\"\\nSearching for best hyperparameters...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_distributions,\n",
    "        n_iter=30,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train, cat_features=cat_features)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best AUC: {random_search.best_score_:.4f}\")\n",
    "    print(f\"‚úÖ Best parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "# ==================== USAGE EXAMPLES ====================\n",
    "\n",
    "\n",
    "# EXAMPLE 1: Basic Usage\n",
    "# ----------------------\n",
    "data = pd.read_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/final.csv')\n",
    "model, importance, cv_results = main_catboost_pipeline(data)\n",
    "\n",
    "# Make predictions\n",
    "# predictions = model.predict_proba(X_new)[:, 1]\n",
    "\n",
    "\n",
    "# EXAMPLE 2: With Hyperparameter Tuning\n",
    "# --------------------------------------\n",
    "# X, y, cat_features = preprocess_for_catboost(data)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# # Tune hyperparameters\n",
    "# best_model, best_params = tune_catboost(X_train, y_train, cat_features)\n",
    "\n",
    "# # Evaluate\n",
    "# y_pred = best_model.predict_proba(X_test)[:, 1]\n",
    "# print(f\"Tuned model AUC: {roc_auc_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "\n",
    "# EXAMPLE 3: Quick Training\n",
    "# --------------------------\n",
    "# X, y, cat_features = preprocess_for_catboost(data)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# model = CatBoostClassifier(\n",
    "#     iterations=1000,\n",
    "#     learning_rate=0.03,\n",
    "#     depth=7,\n",
    "#     auto_class_weights='Balanced',\n",
    "#     cat_features=cat_features,\n",
    "#     random_seed=42,\n",
    "#     verbose=100\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "# print(f\"AUC: {roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba5fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CATBOOST CREDIT DEFAULT PREDICTION\n",
      "======================================================================\n",
      "======================================================================\n",
      "PREPROCESSING FOR CATBOOST\n",
      "======================================================================\n",
      "\n",
      "[1] Initial Dataset:\n",
      "   Samples: 89,999\n",
      "   Features: 67\n",
      "   Default rate: 5.10%\n",
      "\n",
      "[2] Handling missing values...\n",
      "\n",
      "[3] Capping outliers...\n",
      "\n",
      "[4] Engineering features...\n",
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "   Final features: 79\n",
      "   Categorical features: 0\n",
      "   Categorical columns: []\n",
      "\n",
      "======================================================================\n",
      "DATA SPLIT\n",
      "======================================================================\n",
      "\n",
      "Training set: 89,909 samples (5.10% default)\n",
      "Test set:     90 samples (5.56% default)\n",
      "\n",
      "======================================================================\n",
      "CATBOOST MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Class imbalance: 18.59:1\n",
      "\n",
      "Model hyperparameters:\n",
      "   iterations: 1000\n",
      "   learning_rate: 0.03\n",
      "   depth: 7\n",
      "   l2_leaf_reg: 3\n",
      "   min_data_in_leaf: 20\n",
      "   max_bin: 254\n",
      "   subsample: 0.8\n",
      "   colsample_bylevel: 0.8\n",
      "   random_strength: 1\n",
      "   bagging_temperature: 1\n",
      "   auto_class_weights: Balanced\n",
      "   eval_metric: AUC\n",
      "   early_stopping_rounds: 50\n",
      "\n",
      "[1] Training CatBoost...\n",
      "0:\ttest: 0.8317647\tbest: 0.8317647 (0)\ttotal: 19.7ms\tremaining: 19.7s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.8317647059\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "\n",
      "   Best iteration: 0\n",
      "   Best validation AUC: 0.8318\n",
      "\n",
      "[2] Model Evaluation:\n",
      "   Training AUC:   0.7727\n",
      "   Test AUC:       0.8318\n",
      "   Overfitting:    -0.0590\n",
      "\n",
      "   Competition Score: ‚úÖ VERY GOOD (23-24/25 points)\n",
      "\n",
      "[3] Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Default      0.984     0.706     0.822        85\n",
      "     Default      0.138     0.800     0.235         5\n",
      "\n",
      "    accuracy                          0.711        90\n",
      "   macro avg      0.561     0.753     0.529        90\n",
      "weighted avg      0.937     0.711     0.789        90\n",
      "\n",
      "[4] Confusion Matrix:\n",
      "   True Negatives:  60\n",
      "   False Positives: 25\n",
      "   False Negatives: 1\n",
      "   True Positives:  4\n",
      "\n",
      "[5] Business Metrics:\n",
      "   Sensitivity: 80.00% - Caught 80% of defaults\n",
      "   Specificity: 70.59% - Correctly identified 71% of non-defaults\n",
      "   Precision:   13.79% - 14% of predicted defaults were correct\n",
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Performing 5-fold stratified cross-validation...\n",
      "Training on fold [0/5]\n",
      "\n",
      "bestTest = 0.8011012239\n",
      "bestIteration = 273\n",
      "\n",
      "Training on fold [1/5]\n",
      "\n",
      "bestTest = 0.8063020085\n",
      "bestIteration = 359\n",
      "\n",
      "Training on fold [2/5]\n",
      "\n",
      "bestTest = 0.8079253984\n",
      "bestIteration = 122\n",
      "\n",
      "Training on fold [3/5]\n",
      "\n",
      "bestTest = 0.8040079409\n",
      "bestIteration = 334\n",
      "\n",
      "Training on fold [4/5]\n",
      "\n",
      "bestTest = 0.8011635329\n",
      "bestIteration = 193\n",
      "\n",
      "\n",
      "CV Results:\n",
      "   Best iteration: 283\n",
      "   Best CV AUC:    0.8038\n",
      "   Final CV AUC:   0.8035\n",
      "   Std Dev:        0.0029\n",
      "\n",
      "======================================================================\n",
      "TOP 20 MOST IMPORTANT FEATURES\n",
      "======================================================================\n",
      "\n",
      "                        feature  importance\n",
      "              credit_score_norm   49.820502\n",
      "                 loan_to_income   21.240827\n",
      "                 monthly_income   15.267231\n",
      "         monthly_free_cash_flow   12.434740\n",
      "              account_open_year    1.236700\n",
      "                   credit_score    0.000000\n",
      "             loan_purpose_Other    0.000000\n",
      "      employment_type_Full-time    0.000000\n",
      "     origination_channel_Online    0.000000\n",
      "origination_channel_Direct Mail    0.000000\n",
      "     origination_channel_Broker    0.000000\n",
      "  loan_purpose_Revolving Credit    0.000000\n",
      "         loan_purpose_Refinance    0.000000\n",
      "           loan_purpose_Medical    0.000000\n",
      "  employment_type_Self-employed    0.000000\n",
      "    loan_purpose_Major Purchase    0.000000\n",
      "     loan_purpose_Home Purchase    0.000000\n",
      "  loan_purpose_Home Improvement    0.000000\n",
      "             loan_type_Personal    0.000000\n",
      "             loan_type_Mortgage    0.000000\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Test AUC:           0.8318\n",
      "‚úÖ Cross-Val AUC:      0.8038\n",
      "‚úÖ Best Iteration:     0\n",
      "‚úÖ Total Features:     79\n",
      "‚úÖ Categorical Feats:  0\n",
      "\n",
      "üéØ TARGET ACHIEVED! Ready for competition.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CatBoost Credit Default Prediction Pipeline\n",
    "Optimized for 5.1% imbalanced dataset\n",
    "Target: 80%+ AUC score\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== PREPROCESSING FOR CATBOOST ====================\n",
    "\n",
    "def preprocess_for_catboost(df, target='default'):\n",
    "    \"\"\"\n",
    "    CatBoost-specific preprocessing\n",
    "    Advantage: CatBoost handles categorical features natively!\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREPROCESSING FOR CATBOOST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Separate target\n",
    "    y = df[target].copy()\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    print(f\"\\n[1] Initial Dataset:\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Default rate: {y.mean():.2%}\")\n",
    "    \n",
    "    # Drop identifiers and noise\n",
    "    drop_cols = ['customer_id', 'application_id', 'loan_officer_id', \n",
    "                 'random_noise_1', 'recent_inquiry_count', 'oldest_credit_line_age']\n",
    "    X = X.drop([col for col in drop_cols if col in X.columns], axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\n[2] Handling missing values...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                X[f'{col}_missing'] = X[col].isnull().astype(int)\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Cap outliers\n",
    "    print(f\"\\n[3] Capping outliers...\")\n",
    "    outlier_features = ['loan_amount', 'annual_income', 'total_credit_limit',\n",
    "                        'revolving_balance', 'annual_debt_payment']\n",
    "    for col in outlier_features:\n",
    "        if col in X.columns:\n",
    "            lower, upper = X[col].quantile([0.01, 0.99])\n",
    "            X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(f\"\\n[4] Engineering features...\")\n",
    "    \n",
    "    # Debt features\n",
    "    if all(c in X.columns for c in ['debt_to_income_ratio', 'payment_to_income_ratio']):\n",
    "        X['total_debt_burden'] = X['debt_to_income_ratio'] + X['payment_to_income_ratio']\n",
    "        X['debt_stress'] = (X['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "    \n",
    "    # Credit score features\n",
    "    if 'credit_score' in X.columns:\n",
    "        X['credit_score_norm'] = (X['credit_score'] - 300) / 550\n",
    "        X['poor_credit'] = (X['credit_score'] < 650).astype(int)\n",
    "        X['excellent_credit'] = (X['credit_score'] > 750).astype(int)\n",
    "    \n",
    "    # Delinquency features\n",
    "    if 'num_delinquencies_2yrs' in X.columns:\n",
    "        X['has_delinquency'] = (X['num_delinquencies_2yrs'] > 0).astype(int)\n",
    "    \n",
    "    # Utilization\n",
    "    if 'credit_utilization' in X.columns:\n",
    "        X['high_utilization'] = (X['credit_utilization'] > 0.75).astype(int)\n",
    "    \n",
    "    # Income adequacy\n",
    "    if 'annual_income' in X.columns and 'loan_amount' in X.columns:\n",
    "        X['loan_to_income'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "    \n",
    "    # Employment\n",
    "    if 'employment_length' in X.columns:\n",
    "        X['stable_employment'] = (X['employment_length'] >= 3).astype(int)\n",
    "    \n",
    "    # Account age\n",
    "    if 'oldest_account_age_months' in X.columns:\n",
    "        X['thin_credit_file'] = (X['oldest_account_age_months'] < 24).astype(int)\n",
    "    \n",
    "    # Inquiries\n",
    "    if 'num_inquiries_6mo' in X.columns:\n",
    "        X['excessive_inquiries'] = (X['num_inquiries_6mo'] > 4).astype(int)\n",
    "    \n",
    "    # Interest rate\n",
    "    if 'interest_rate' in X.columns:\n",
    "        X['subprime_rate'] = (X['interest_rate'] > 15).astype(int)\n",
    "    \n",
    "    # Identify categorical features for CatBoost\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # CatBoost handles categoricals, so DON'T one-hot encode!\n",
    "    # Just convert to 'category' dtype\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].astype('category')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   Final features: {X.shape[1]}\")\n",
    "    print(f\"   Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"   Categorical columns: {categorical_features}\")\n",
    "    \n",
    "    return X, y, categorical_features\n",
    "\n",
    "# ==================== CATBOOST TRAINING ====================\n",
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test, cat_features=None):\n",
    "    \"\"\"\n",
    "    Train CatBoost with optimal hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATBOOST MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f\"\\nClass imbalance: {scale_pos_weight:.2f}:1\")\n",
    "    \n",
    "    # CatBoost parameters (optimized for credit default)\n",
    "    params = {\n",
    "        'iterations': 1000,              # Max iterations\n",
    "        'learning_rate': 0.03,           # Learning rate\n",
    "        'depth': 7,                      # Tree depth\n",
    "        'l2_leaf_reg': 3,                # L2 regularization\n",
    "        'min_data_in_leaf': 20,          # Min samples per leaf\n",
    "        'max_bin': 254,                  # Max bins for numerical features\n",
    "        'subsample': 0.8,                # Row sampling\n",
    "        'colsample_bylevel': 0.8,        # Column sampling per level\n",
    "        'random_strength': 1,            # Randomness for scoring splits\n",
    "        'bagging_temperature': 1,        # Bayesian bootstrap temperature\n",
    "        'auto_class_weights': 'Balanced', # Handle imbalance automatically\n",
    "        'eval_metric': 'AUC',            # Evaluation metric\n",
    "        'early_stopping_rounds': 50,     # Early stopping\n",
    "        'random_seed': 42,\n",
    "        'verbose': 100,                  # Print every 100 iterations\n",
    "        'task_type': 'CPU',              # Use GPU if available: 'GPU'\n",
    "        'thread_count': -1               # Use all CPU cores\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nModel hyperparameters:\")\n",
    "    for key, value in params.items():\n",
    "        if key not in ['verbose', 'task_type', 'thread_count', 'random_seed']:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Create CatBoost model\n",
    "    model = CatBoostClassifier(**params)\n",
    "    \n",
    "    # Train with validation set\n",
    "    print(f\"\\n[1] Training CatBoost...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        cat_features=cat_features,      # Specify categorical features\n",
    "        eval_set=(X_test, y_test),\n",
    "        use_best_model=True,             # Use best iteration\n",
    "        plot=False                       # Set True for training visualization\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n   Best iteration: {model.best_iteration_}\")\n",
    "    print(f\"   Best validation AUC: {model.best_score_['validation']['AUC']:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n[2] Model Evaluation:\")\n",
    "    \n",
    "    # Training performance\n",
    "    y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    \n",
    "    # Test performance\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"   Training AUC:   {train_auc:.4f}\")\n",
    "    print(f\"   Test AUC:       {test_auc:.4f}\")\n",
    "    print(f\"   Overfitting:    {train_auc - test_auc:.4f}\")\n",
    "    \n",
    "    # Competition scoring\n",
    "    if test_auc >= 0.85:\n",
    "        grade = \"üèÜ EXCELLENT (25/25 points)\"\n",
    "    elif test_auc >= 0.80:\n",
    "        grade = \"‚úÖ VERY GOOD (23-24/25 points)\"\n",
    "    elif test_auc >= 0.75:\n",
    "        grade = \"‚úÖ GOOD (20-22/25 points)\"\n",
    "    elif test_auc >= 0.60:\n",
    "        grade = \"‚ö†Ô∏è  PASSING (15-19/25 points)\"\n",
    "    else:\n",
    "        grade = \"‚ùå DISQUALIFIED (<15 points)\"\n",
    "    \n",
    "    print(f\"\\n   Competition Score: {grade}\")\n",
    "    \n",
    "    # Classification report\n",
    "    y_test_class = model.predict(X_test)\n",
    "    print(f\"\\n[3] Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_class,\n",
    "                                target_names=['No Default', 'Default'],\n",
    "                                digits=3))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"[4] Confusion Matrix:\")\n",
    "    print(f\"   True Negatives:  {tn:,}\")\n",
    "    print(f\"   False Positives: {fp:,}\")\n",
    "    print(f\"   False Negatives: {fn:,}\")\n",
    "    print(f\"   True Positives:  {tp:,}\")\n",
    "    \n",
    "    # Business metrics\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n[5] Business Metrics:\")\n",
    "    print(f\"   Sensitivity: {sensitivity:.2%} - Caught {sensitivity:.0%} of defaults\")\n",
    "    print(f\"   Specificity: {specificity:.2%} - Correctly identified {specificity:.0%} of non-defaults\")\n",
    "    print(f\"   Precision:   {precision:.2%} - {precision:.0%} of predicted defaults were correct\")\n",
    "    \n",
    "    return model, test_auc\n",
    "\n",
    "# ==================== CROSS-VALIDATION ====================\n",
    "\n",
    "def catboost_cross_validate(X, y, cat_features=None):\n",
    "    \"\"\"\n",
    "    5-fold stratified cross-validation with CatBoost\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create Pool object (CatBoost's data structure)\n",
    "    pool = Pool(\n",
    "        data=X,\n",
    "        label=y,\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "    \n",
    "    # CV parameters\n",
    "    params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'depth': 7,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'auto_class_weights': 'Balanced',\n",
    "        'loss_function': 'Logloss',      # ‚úÖ REQUIRED for cv()\n",
    "        'eval_metric': 'AUC',\n",
    "        'random_seed': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    # Perform 5-fold CV\n",
    "    print(f\"\\nPerforming 5-fold stratified cross-validation...\")\n",
    "    cv_results = cv(\n",
    "        pool=pool,\n",
    "        params=params,\n",
    "        fold_count=5,\n",
    "        stratified=True,\n",
    "        partition_random_seed=42,\n",
    "        shuffle=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Extract AUC scores\n",
    "    cv_scores = cv_results['test-AUC-mean'].values\n",
    "    best_iteration = cv_results['test-AUC-mean'].idxmax()\n",
    "    \n",
    "    print(f\"\\nCV Results:\")\n",
    "    print(f\"   Best iteration: {best_iteration}\")\n",
    "    print(f\"   Best CV AUC:    {cv_scores[best_iteration]:.4f}\")\n",
    "    print(f\"   Final CV AUC:   {cv_scores[-1]:.4f}\")\n",
    "    print(f\"   Std Dev:        {cv_results['test-AUC-std'].values[best_iteration]:.4f}\")\n",
    "    \n",
    "    return cv_scores[best_iteration], cv_results\n",
    "\n",
    "# ==================== FEATURE IMPORTANCE ====================\n",
    "\n",
    "def plot_feature_importance(model, X, top_n=20):\n",
    "    \"\"\"\n",
    "    Display feature importance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP {top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{importance_df.head(top_n).to_string(index=False)}\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def main_catboost_pipeline(df):\n",
    "    \"\"\"\n",
    "    Complete CatBoost pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATBOOST CREDIT DEFAULT PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Preprocess\n",
    "    X, y, cat_features = preprocess_for_catboost(df)\n",
    "    \n",
    "    # Split data (stratified)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.001,\n",
    "        stratify=y,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTraining set: {X_train.shape[0]:,} samples ({y_train.mean():.2%} default)\")\n",
    "    print(f\"Test set:     {X_test.shape[0]:,} samples ({y_test.mean():.2%} default)\")\n",
    "    \n",
    "    # Train model\n",
    "    model, test_auc = train_catboost(X_train, y_train, X_test, y_test, cat_features)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_auc, cv_results = catboost_cross_validate(X_train, y_train, cat_features)\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = plot_feature_importance(model, X_train, top_n=20)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚úÖ Test AUC:           {test_auc:.4f}\")\n",
    "    print(f\"‚úÖ Cross-Val AUC:      {cv_auc:.4f}\")\n",
    "    print(f\"‚úÖ Best Iteration:     {model.best_iteration_}\")\n",
    "    print(f\"‚úÖ Total Features:     {X_train.shape[1]}\")\n",
    "    print(f\"‚úÖ Categorical Feats:  {len(cat_features)}\")\n",
    "    \n",
    "    if test_auc >= 0.80:\n",
    "        print(f\"\\nüéØ TARGET ACHIEVED! Ready for competition.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  AUC below 80%. Consider hyperparameter tuning.\")\n",
    "    \n",
    "    return model, importance_df, cv_results\n",
    "\n",
    "# ==================== HYPERPARAMETER TUNING ====================\n",
    "\n",
    "def tune_catboost(X_train, y_train, cat_features=None):\n",
    "    \"\"\"\n",
    "    Grid search for optimal hyperparameters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    \n",
    "    # Parameter grid\n",
    "    param_distributions = {\n",
    "        'depth': [5, 6, 7, 8, 9],\n",
    "        'learning_rate': [0.01, 0.02, 0.03, 0.05],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7],\n",
    "        'min_data_in_leaf': [10, 20, 30],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bylevel': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    \n",
    "    # Base model\n",
    "    base_model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        auto_class_weights='Balanced',\n",
    "        eval_metric='AUC',\n",
    "        early_stopping_rounds=50,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Randomized search\n",
    "    print(\"\\nSearching for best hyperparameters...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_model,\n",
    "        param_distributions,\n",
    "        n_iter=30,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train, cat_features=cat_features)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best AUC: {random_search.best_score_:.4f}\")\n",
    "    print(f\"‚úÖ Best parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.best_params_\n",
    "\n",
    "# ==================== USAGE EXAMPLES ====================\n",
    "\n",
    "# EXAMPLE 1: Basic Usage\n",
    "# ----------------------\n",
    "data = pd.read_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/final.csv')\n",
    "model, importance, cv_results = main_catboost_pipeline(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cc8f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CatBoost Prediction Pipeline\n",
    "Predict on new data and save results\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_new_data(df, cat_features_from_training):\n",
    "    \"\"\"\n",
    "    Apply the SAME preprocessing steps to new data\n",
    "    Must match training preprocessing exactly!\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREPROCESSING NEW DATA FOR PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    X = df.copy()\n",
    "    \n",
    "    print(f\"\\n[1] Initial Dataset:\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    \n",
    "    # Store IDs if present (for results file)\n",
    "    id_columns = {}\n",
    "    if 'customer_id' in X.columns:\n",
    "        id_columns['customer_id'] = X['customer_id'].copy()\n",
    "    if 'application_id' in X.columns:\n",
    "        id_columns['application_id'] = X['application_id'].copy()\n",
    "    \n",
    "    # Drop identifiers and noise (same as training)\n",
    "    drop_cols = ['customer_id', 'application_id', 'loan_officer_id', \n",
    "                 'random_noise_1', 'recent_inquiry_count', 'oldest_credit_line_age']\n",
    "    X = X.drop([col for col in drop_cols if col in X.columns], axis=1)\n",
    "    \n",
    "    # Handle missing values (same as training)\n",
    "    print(f\"\\n[2] Handling missing values...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                X[f'{col}_missing'] = X[col].isnull().astype(int)\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Cap outliers (same as training)\n",
    "    print(f\"\\n[3] Capping outliers...\")\n",
    "    outlier_features = ['loan_amount', 'annual_income', 'total_credit_limit',\n",
    "                        'revolving_balance', 'annual_debt_payment']\n",
    "    for col in outlier_features:\n",
    "        if col in X.columns:\n",
    "            lower, upper = X[col].quantile([0.01, 0.99])\n",
    "            X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Feature engineering (same as training)\n",
    "    print(f\"\\n[4] Engineering features...\")\n",
    "    \n",
    "    # Debt features\n",
    "    if all(c in X.columns for c in ['debt_to_income_ratio', 'payment_to_income_ratio']):\n",
    "        X['total_debt_burden'] = X['debt_to_income_ratio'] + X['payment_to_income_ratio']\n",
    "        X['debt_stress'] = (X['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "    \n",
    "    # Credit score features\n",
    "    if 'credit_score' in X.columns:\n",
    "        X['credit_score_norm'] = (X['credit_score'] - 300) / 550\n",
    "        X['poor_credit'] = (X['credit_score'] < 650).astype(int)\n",
    "        X['excellent_credit'] = (X['credit_score'] > 750).astype(int)\n",
    "    \n",
    "    # Delinquency features\n",
    "    if 'num_delinquencies_2yrs' in X.columns:\n",
    "        X['has_delinquency'] = (X['num_delinquencies_2yrs'] > 0).astype(int)\n",
    "    \n",
    "    # Utilization\n",
    "    if 'credit_utilization' in X.columns:\n",
    "        X['high_utilization'] = (X['credit_utilization'] > 0.75).astype(int)\n",
    "    \n",
    "    # Income adequacy\n",
    "    if 'annual_income' in X.columns and 'loan_amount' in X.columns:\n",
    "        X['loan_to_income'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "    \n",
    "    # Employment\n",
    "    if 'employment_length' in X.columns:\n",
    "        X['stable_employment'] = (X['employment_length'] >= 3).astype(int)\n",
    "    \n",
    "    # Account age\n",
    "    if 'oldest_account_age_months' in X.columns:\n",
    "        X['thin_credit_file'] = (X['oldest_account_age_months'] < 24).astype(int)\n",
    "    \n",
    "    # Inquiries\n",
    "    if 'num_inquiries_6mo' in X.columns:\n",
    "        X['excessive_inquiries'] = (X['num_inquiries_6mo'] > 4).astype(int)\n",
    "    \n",
    "    # Interest rate\n",
    "    if 'interest_rate' in X.columns:\n",
    "        X['subprime_rate'] = (X['interest_rate'] > 15).astype(int)\n",
    "    \n",
    "    # Convert categorical features to 'category' dtype (same as training)\n",
    "    for col in cat_features_from_training:\n",
    "        if col in X.columns:\n",
    "            X[col] = X[col].astype('category')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   Final features: {X.shape[1]}\")\n",
    "    \n",
    "    return X, id_columns\n",
    "\n",
    "\n",
    "def predict_and_save(model, new_data_path, output_path='results.csv', \n",
    "                     cat_features=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict on new data and save results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : CatBoostClassifier\n",
    "        Trained CatBoost model\n",
    "    new_data_path : str\n",
    "        Path to new data CSV file\n",
    "    output_path : str\n",
    "        Path to save results CSV (default: 'results.csv')\n",
    "    cat_features : list\n",
    "        List of categorical feature names from training\n",
    "    threshold : float\n",
    "        Classification threshold (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame with predictions\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CATBOOST PREDICTION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load new data\n",
    "    print(f\"\\n[1] Loading new data from: {new_data_path}\")\n",
    "    new_df = pd.read_csv(new_data_path)\n",
    "    print(f\"   Loaded {len(new_df):,} samples\")\n",
    "    \n",
    "    # Preprocess new data (same as training)\n",
    "    X_new, id_columns = preprocess_new_data(new_df, cat_features or [])\n",
    "    \n",
    "    # Make predictions\n",
    "    print(f\"\\n[2] Generating predictions...\")\n",
    "    \n",
    "    # Predicted probabilities\n",
    "    y_pred_proba = model.predict_proba(X_new)[:, 1]\n",
    "    \n",
    "    # Predicted classes\n",
    "    y_pred_class = model.predict(X_new)\n",
    "    \n",
    "    # Alternative: use custom threshold\n",
    "    y_pred_class_custom = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    print(f\"   ‚úÖ Predictions generated!\")\n",
    "    print(f\"   Predicted default rate: {y_pred_class.mean():.2%}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    print(f\"\\n[3] Creating results file...\")\n",
    "    \n",
    "    results_df = pd.DataFrame()\n",
    "    \n",
    "    # Add IDs if available\n",
    "    for id_col, id_values in id_columns.items():\n",
    "        results_df[id_col] = id_values.values\n",
    "    \n",
    "    # Add predictions\n",
    "    results_df['predicted_probability'] = y_pred_proba\n",
    "    results_df['predicted_default'] = y_pred_class\n",
    "    results_df['predicted_default_custom'] = y_pred_class_custom\n",
    "    \n",
    "    # Add risk categories\n",
    "    results_df['risk_category'] = pd.cut(\n",
    "        y_pred_proba,\n",
    "        bins=[0, 0.25, 0.5, 0.75, 1.0],\n",
    "        labels=['Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']\n",
    "    )\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"   ‚úÖ Results saved to: {output_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n[4] Prediction Summary:\")\n",
    "    print(f\"   Total predictions:     {len(results_df):,}\")\n",
    "    print(f\"   Predicted defaults:    {y_pred_class.sum():,} ({y_pred_class.mean():.2%})\")\n",
    "    print(f\"   Predicted non-default: {(1-y_pred_class).sum():,} ({(1-y_pred_class).mean():.2%})\")\n",
    "    print(f\"\\n   Risk Distribution:\")\n",
    "    print(results_df['risk_category'].value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\n   Probability Statistics:\")\n",
    "    print(f\"   Min:    {y_pred_proba.min():.4f}\")\n",
    "    print(f\"   25th:   {np.percentile(y_pred_proba, 25):.4f}\")\n",
    "    print(f\"   Median: {np.median(y_pred_proba):.4f}\")\n",
    "    print(f\"   75th:   {np.percentile(y_pred_proba, 75):.4f}\")\n",
    "    print(f\"   Max:    {y_pred_proba.max():.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREDICTION COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ==================== USAGE EXAMPLE ====================\n",
    "\n",
    "# After training your model with the original code:\n",
    "# model, importance, cv_results = main_catboost_pipeline(data)\n",
    "\n",
    "# Get categorical features from training\n",
    "# (you need to save these from the training pipeline)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Load training data and train model\n",
    "train_data = pd.read_csv('data/final.csv')\n",
    "model, importance, cv_results = main_catboost_pipeline(train_data)\n",
    "\n",
    "# Get categorical features (from training preprocessing)\n",
    "from your_training_script import preprocess_for_catboost\n",
    "_, _, cat_features = preprocess_for_catboost(train_data)\n",
    "\n",
    "# Predict on new data\n",
    "results = predict_and_save(\n",
    "    model=model,\n",
    "    new_data_path='data/new_data.csv',\n",
    "    output_path='results.csv',\n",
    "    cat_features=cat_features,\n",
    "    threshold=0.5  # Adjust threshold as needed\n",
    ")\n",
    "\n",
    "# View first few predictions\n",
    "print(results.head(10))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ==================== COMPLETE PIPELINE ====================\n",
    "\n",
    "def complete_pipeline_with_prediction(train_data_path, test_data_path, output_path='results.csv'):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Train model and predict on new data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_data_path : str\n",
    "        Path to training data CSV\n",
    "    test_data_path : str\n",
    "        Path to test data CSV for prediction\n",
    "    output_path : str\n",
    "        Path to save predictions\n",
    "    \"\"\"\n",
    "    from your_training_script import main_catboost_pipeline, preprocess_for_catboost\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE CATBOOST PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # STEP 1: Load and train\n",
    "    print(\"\\n[STEP 1] Training model...\")\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    model, importance, cv_results = main_catboost_pipeline(train_df)\n",
    "    \n",
    "    # STEP 2: Get categorical features\n",
    "    print(\"\\n[STEP 2] Extracting categorical features...\")\n",
    "    _, _, cat_features = preprocess_for_catboost(train_df)\n",
    "    \n",
    "    # STEP 3: Predict on new data\n",
    "    print(\"\\n[STEP 3] Predicting on new data...\")\n",
    "    results = predict_and_save(\n",
    "        model=model,\n",
    "        new_data_path=test_data_path,\n",
    "        output_path=output_path,\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Pipeline complete! Results saved to:\", output_path)\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "\n",
    "# ==================== SIMPLE USAGE ====================\n",
    "\n",
    "# If you already have the trained model:\n",
    "results = predict_and_save(\n",
    "    model=model,  # Your trained model\n",
    "    new_data_path='new_data.csv',\n",
    "    output_path='results.csv',\n",
    "    cat_features=cat_features  # From training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "363f9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_catboost(df):\n",
    "    \"\"\"\n",
    "    CatBoost-specific preprocessing\n",
    "    Advantage: CatBoost handles categorical features natively!\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREPROCESSING FOR CATBOOST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Separate target\n",
    "    X = df\n",
    "    \n",
    "    print(f\"\\n[1] Initial Dataset:\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    # print(f\"   Default rate: {y.mean():.2%}\")\n",
    "    \n",
    "    # Drop identifiers and noise\n",
    "    drop_cols = ['customer_id', 'application_id', 'loan_officer_id', \n",
    "                 'random_noise_1', 'recent_inquiry_count', 'oldest_credit_line_age']\n",
    "    X = X.drop([col for col in drop_cols if col in X.columns], axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\n[2] Handling missing values...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                X[f'{col}_missing'] = X[col].isnull().astype(int)\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Cap outliers\n",
    "    print(f\"\\n[3] Capping outliers...\")\n",
    "    outlier_features = ['loan_amount', 'annual_income', 'total_credit_limit',\n",
    "                        'revolving_balance', 'annual_debt_payment']\n",
    "    for col in outlier_features:\n",
    "        if col in X.columns:\n",
    "            lower, upper = X[col].quantile([0.01, 0.99])\n",
    "            X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(f\"\\n[4] Engineering features...\")\n",
    "    \n",
    "    # Debt features\n",
    "    if all(c in X.columns for c in ['debt_to_income_ratio', 'payment_to_income_ratio']):\n",
    "        X['total_debt_burden'] = X['debt_to_income_ratio'] + X['payment_to_income_ratio']\n",
    "        X['debt_stress'] = (X['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "    \n",
    "    # Credit score features\n",
    "    if 'credit_score' in X.columns:\n",
    "        X['credit_score_norm'] = (X['credit_score'] - 300) / 550\n",
    "        X['poor_credit'] = (X['credit_score'] < 650).astype(int)\n",
    "        X['excellent_credit'] = (X['credit_score'] > 750).astype(int)\n",
    "    \n",
    "    # Delinquency features\n",
    "    if 'num_delinquencies_2yrs' in X.columns:\n",
    "        X['has_delinquency'] = (X['num_delinquencies_2yrs'] > 0).astype(int)\n",
    "    \n",
    "    # Utilization\n",
    "    if 'credit_utilization' in X.columns:\n",
    "        X['high_utilization'] = (X['credit_utilization'] > 0.75).astype(int)\n",
    "    \n",
    "    # Income adequacy\n",
    "    if 'annual_income' in X.columns and 'loan_amount' in X.columns:\n",
    "        X['loan_to_income'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "    \n",
    "    # Employment\n",
    "    if 'employment_length' in X.columns:\n",
    "        X['stable_employment'] = (X['employment_length'] >= 3).astype(int)\n",
    "    \n",
    "    # Account age\n",
    "    if 'oldest_account_age_months' in X.columns:\n",
    "        X['thin_credit_file'] = (X['oldest_account_age_months'] < 24).astype(int)\n",
    "    \n",
    "    # Inquiries\n",
    "    if 'num_inquiries_6mo' in X.columns:\n",
    "        X['excessive_inquiries'] = (X['num_inquiries_6mo'] > 4).astype(int)\n",
    "    \n",
    "    # Interest rate\n",
    "    if 'interest_rate' in X.columns:\n",
    "        X['subprime_rate'] = (X['interest_rate'] > 15).astype(int)\n",
    "    \n",
    "    # Identify categorical features for CatBoost\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # CatBoost handles categoricals, so DON'T one-hot encode!\n",
    "    # Just convert to 'category' dtype\n",
    "    for col in categorical_features:\n",
    "        X[col] = X[col].astype('category')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   Final features: {X.shape[1]}\")\n",
    "    print(f\"   Categorical features: {len(categorical_features)}\")\n",
    "    print(f\"   Categorical columns: {categorical_features}\")\n",
    "    \n",
    "    return X, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87fea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cee5b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPROCESSING FOR CATBOOST\n",
      "======================================================================\n",
      "\n",
      "[1] Initial Dataset:\n",
      "   Samples: 10,001\n",
      "   Features: 67\n",
      "\n",
      "[2] Handling missing values...\n",
      "\n",
      "[3] Capping outliers...\n",
      "\n",
      "[4] Engineering features...\n",
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "   Final features: 79\n",
      "   Categorical features: 0\n",
      "   Categorical columns: []\n"
     ]
    }
   ],
   "source": [
    "final_test = pd.read_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/evaluation_set/final_test.csv')\n",
    "X, test_cat = preprocess_for_catboost(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2db7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = model.predict(X).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4cccc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = model.predict_proba(X).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40ea0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "for i in l:\n",
    "    output.append(round(max(i), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45791bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52727"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fac60d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/evaluation_set/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c8c62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_df['customer_id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad524f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'customer_id': ids, 'prob': output, 'default': default}).to_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec7289a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d20a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GRADIENT BOOSTING CREDIT DEFAULT PREDICTION\n",
      "======================================================================\n",
      "======================================================================\n",
      "PREPROCESSING FOR GRADIENT BOOSTING\n",
      "======================================================================\n",
      "\n",
      "[1] Initial Dataset:\n",
      "   Samples: 89,999\n",
      "   Features: 67\n",
      "   Default rate: 5.10%\n",
      "   Imbalance ratio: 18.6:1\n",
      "\n",
      "[2] Dropped 0 ID/noise columns\n",
      "\n",
      "[3] Handling missing values...\n",
      "   Handled 0 features with missing values\n",
      "\n",
      "[4] Capping outliers at 1st and 99th percentiles...\n",
      "\n",
      "[5] Engineering features...\n",
      "   Created 23 new features\n",
      "\n",
      "[6] Encoding categorical variables...\n",
      "\n",
      "‚úÖ Preprocessing complete!\n",
      "   Final features: 89\n",
      "   Ready for modeling\n",
      "\n",
      "======================================================================\n",
      "DATA SPLIT\n",
      "======================================================================\n",
      "\n",
      "Training: 71,999 samples (5.10% default)\n",
      "Test:     18,000 samples (5.11% default)\n",
      "\n",
      "======================================================================\n",
      "SKLEARN GRADIENT BOOSTING\n",
      "======================================================================\n",
      "\n",
      "Using sample weights to handle imbalance\n",
      "   Default weight: 9.80\n",
      "   No default weight: 0.53\n",
      "\n",
      "[1] Training Sklearn GradientBoosting...\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           1.3578           0.0240           54.50s\n",
      "         2           1.3329           0.0232           52.40s\n",
      "         3           1.3101           0.0218           49.09s\n",
      "         4           1.2903           0.0256           51.00s\n",
      "         5           1.2699           0.0169           50.00s\n",
      "         6           1.2499           0.0166           51.48s\n",
      "         7           1.2310           0.0102           52.59s\n",
      "         8           1.2165           0.0240           52.15s\n",
      "         9           1.2029           0.0179           52.02s\n",
      "        10           1.1879           0.0091           52.39s\n",
      "        20           1.0844          -0.0087           48.57s\n",
      "        30           1.0194          -0.0218           46.96s\n",
      "        40           0.9838          -0.0057           45.31s\n",
      "        50           0.9558           0.0262           44.30s\n",
      "        60           0.9231           0.0053           43.33s\n",
      "        70           0.9077           0.0428           42.26s\n",
      "        80           0.8775          -0.0189           41.37s\n",
      "        90           0.8624           0.0103           40.43s\n",
      "   Training stopped at iteration: 97\n",
      "\n",
      "[2] Model Evaluation:\n",
      "   Training AUC:   0.8933\n",
      "   Test AUC:       0.8006\n",
      "   Overfitting:    0.0927\n",
      "\n",
      "   Competition Score: ‚úÖ VERY GOOD (23-24/25 points)\n",
      "\n",
      "======================================================================\n",
      "5-FOLD CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Performing 5-fold stratified cross-validation...\n",
      "\n",
      "CV AUC Scores by Fold:\n",
      "   Fold 1: 0.7952\n",
      "   Fold 2: 0.8125\n",
      "   Fold 3: 0.7901\n",
      "   Fold 4: 0.7881\n",
      "   Fold 5: 0.7992\n",
      "\n",
      "   Mean CV AUC: 0.7970\n",
      "   Std Dev:     0.0087\n",
      "   Min AUC:     0.7881\n",
      "   Max AUC:     0.8125\n",
      "   95% CI:      [0.7800, 0.8140]\n",
      "\n",
      "======================================================================\n",
      "TOP 20 MOST IMPORTANT FEATURES\n",
      "======================================================================\n",
      "\n",
      "                feature  importance\n",
      "      credit_score_norm    0.109348\n",
      " monthly_free_cash_flow    0.074488\n",
      "                    age    0.065335\n",
      "          annual_income    0.055495\n",
      "     debt_service_ratio    0.050178\n",
      "           credit_score    0.039387\n",
      "            credit_tier    0.036151\n",
      "       available_credit    0.034886\n",
      "      total_debt_burden    0.031341\n",
      "     total_credit_limit    0.026281\n",
      "payment_to_income_ratio    0.023981\n",
      "    num_credit_accounts    0.022843\n",
      "   debt_to_income_ratio    0.022358\n",
      "  loan_to_annual_income    0.022046\n",
      "         loan_to_income    0.021671\n",
      "     credit_utilization    0.020230\n",
      "    annual_debt_payment    0.018095\n",
      "  existing_monthly_debt    0.017822\n",
      "        monthly_payment    0.017106\n",
      "     num_login_sessions    0.016845\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE: XGBoost + LightGBM + CatBoost + Sklearn GB\n",
      "======================================================================\n",
      "\n",
      "[1] Training individual models...\n",
      "\n",
      "   Training XGBoost...\n",
      "   XGBoost AUC: 0.8043\n",
      "\n",
      "   Training LightGBM...\n",
      "   LightGBM AUC: 0.8029\n",
      "\n",
      "   Training CatBoost...\n",
      "   CatBoost AUC: 0.8097\n",
      "\n",
      "   Training Sklearn GradientBoosting...\n",
      "   Sklearn GB AUC: 0.8023\n",
      "\n",
      "[2] Creating ensemble...\n",
      "\n",
      "[3] Ensemble Results:\n",
      "   Weights: XGB=0.250, LGB=0.249, CAT=0.252, GB=0.249\n",
      "   Ensemble AUC: 0.8098\n",
      "   Improvement: +0.0001\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Sklearn GB Test AUC:  0.8006\n",
      "‚úÖ Ensemble AUC:         0.8098\n",
      "‚úÖ Mean CV AUC:          0.7970 (¬±0.0087)\n",
      "‚úÖ Total Features:       89\n",
      "\n",
      "üéØ TARGET ACHIEVED! Ready for competition.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Make predictions with ensemble\\nX_new_preprocessed, _ = preprocess_for_gradient_boosting(X_new)\\npredictions = (\\n    ensemble_models['xgboost'].predict_proba(X_new_preprocessed)[:, 1] * 0.3 +\\n    ensemble_models['lightgbm'].predict_proba(X_new_preprocessed)[:, 1] * 0.3 +\\n    ensemble_models['catboost'].predict_proba(X_new_preprocessed)[:, 1] * 0.3 +\\n    ensemble_models['sklearn_gb'].predict_proba(X_new_preprocessed)[:, 1] * 0.1\\n)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gradient Boosting Credit Default Prediction Pipeline\n",
    "Includes: Sklearn GradientBoosting, XGBoost, LightGBM, CatBoost\n",
    "Target: 80%+ AUC score\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== PREPROCESSING ====================\n",
    "\n",
    "def preprocess_for_gradient_boosting(df, target='default'):\n",
    "    \"\"\"\n",
    "    Preprocessing for Gradient Boosting models\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PREPROCESSING FOR GRADIENT BOOSTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Separate target\n",
    "    y = df[target].copy()\n",
    "    X = df.drop(target, axis=1)\n",
    "    \n",
    "    print(f\"\\n[1] Initial Dataset:\")\n",
    "    print(f\"   Samples: {len(X):,}\")\n",
    "    print(f\"   Features: {X.shape[1]}\")\n",
    "    print(f\"   Default rate: {y.mean():.2%}\")\n",
    "    print(f\"   Imbalance ratio: {(1-y.mean())/y.mean():.1f}:1\")\n",
    "    \n",
    "    # Drop identifiers and noise\n",
    "    drop_cols = ['customer_id', 'application_id', 'loan_officer_id', \n",
    "                 'random_noise_1', 'recent_inquiry_count', 'oldest_credit_line_age',\n",
    "                 'previous_zip_code', 'marketing_campaign', 'referral_code']\n",
    "    X = X.drop([col for col in drop_cols if col in X.columns], axis=1)\n",
    "    print(f\"\\n[2] Dropped {len([c for c in drop_cols if c in df.columns])} ID/noise columns\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\n[3] Handling missing values...\")\n",
    "    missing_count = 0\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            if X[col].dtype in ['float64', 'int64']:\n",
    "                X[f'{col}_missing'] = X[col].isnull().astype(int)\n",
    "                X[col].fillna(X[col].median(), inplace=True)\n",
    "                missing_count += 1\n",
    "    print(f\"   Handled {missing_count} features with missing values\")\n",
    "    \n",
    "    # Cap outliers\n",
    "    print(f\"\\n[4] Capping outliers at 1st and 99th percentiles...\")\n",
    "    outlier_features = ['loan_amount', 'annual_income', 'total_credit_limit',\n",
    "                        'revolving_balance', 'annual_debt_payment', 'monthly_free_cash_flow']\n",
    "    for col in outlier_features:\n",
    "        if col in X.columns:\n",
    "            lower, upper = X[col].quantile([0.01, 0.99])\n",
    "            X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "    \n",
    "    # Feature engineering\n",
    "    print(f\"\\n[5] Engineering features...\")\n",
    "    engineered_count = 0\n",
    "    \n",
    "    # Debt burden features\n",
    "    if all(c in X.columns for c in ['debt_to_income_ratio', 'payment_to_income_ratio']):\n",
    "        X['total_debt_burden'] = X['debt_to_income_ratio'] + X['payment_to_income_ratio']\n",
    "        X['debt_stress'] = (X['debt_to_income_ratio'] > 0.5).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Credit score features\n",
    "    if 'credit_score' in X.columns:\n",
    "        X['credit_score_norm'] = (X['credit_score'] - 300) / 550\n",
    "        X['poor_credit'] = (X['credit_score'] < 650).astype(int)\n",
    "        X['excellent_credit'] = (X['credit_score'] > 750).astype(int)\n",
    "        X['credit_tier'] = pd.cut(X['credit_score'], \n",
    "                                   bins=[0, 580, 670, 740, 850],\n",
    "                                   labels=[0, 1, 2, 3]).astype(int)  # Convert to int\n",
    "        engineered_count += 4\n",
    "    \n",
    "    # Delinquency features\n",
    "    if 'num_delinquencies_2yrs' in X.columns:\n",
    "        X['has_delinquency'] = (X['num_delinquencies_2yrs'] > 0).astype(int)\n",
    "        X['multiple_delinquencies'] = (X['num_delinquencies_2yrs'] > 1).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Credit utilization\n",
    "    if 'credit_utilization' in X.columns:\n",
    "        X['high_utilization'] = (X['credit_utilization'] > 0.75).astype(int)\n",
    "        X['very_low_utilization'] = (X['credit_utilization'] < 0.1).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Income and loan features\n",
    "    if 'annual_income' in X.columns and 'loan_amount' in X.columns:\n",
    "        X['loan_to_income'] = X['loan_amount'] / (X['annual_income'] + 1)\n",
    "        X['high_loan_burden'] = (X['loan_to_income'] > 3).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Employment stability\n",
    "    if 'employment_length' in X.columns:\n",
    "        X['stable_employment'] = (X['employment_length'] >= 3).astype(int)\n",
    "        X['new_employment'] = (X['employment_length'] < 1).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Account age\n",
    "    if 'oldest_account_age_months' in X.columns:\n",
    "        X['account_age_years'] = X['oldest_account_age_months'] / 12\n",
    "        X['thin_credit_file'] = (X['oldest_account_age_months'] < 24).astype(int)\n",
    "        X['mature_credit'] = (X['oldest_account_age_months'] >= 60).astype(int)\n",
    "        engineered_count += 3\n",
    "    \n",
    "    # Inquiry features\n",
    "    if 'num_inquiries_6mo' in X.columns:\n",
    "        X['credit_shopping'] = (X['num_inquiries_6mo'] > 2).astype(int)\n",
    "        X['excessive_inquiries'] = (X['num_inquiries_6mo'] > 4).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Interest rate\n",
    "    if 'interest_rate' in X.columns:\n",
    "        X['subprime_rate'] = (X['interest_rate'] > 15).astype(int)\n",
    "        X['prime_rate'] = (X['interest_rate'] < 8).astype(int)\n",
    "        engineered_count += 2\n",
    "    \n",
    "    # Collections and public records\n",
    "    if 'num_collections' in X.columns:\n",
    "        X['has_collections'] = (X['num_collections'] > 0).astype(int)\n",
    "        engineered_count += 1\n",
    "    \n",
    "    if 'num_public_records' in X.columns:\n",
    "        X['has_public_records'] = (X['num_public_records'] > 0).astype(int)\n",
    "        engineered_count += 1\n",
    "    \n",
    "    print(f\"   Created {engineered_count} new features\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(f\"\\n[6] Encoding categorical variables...\")\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # One-hot encode low cardinality\n",
    "    low_card = [col for col in categorical_cols if X[col].nunique() < 10]\n",
    "    if low_card:\n",
    "        X = pd.get_dummies(X, columns=low_card, drop_first=True, dtype=int)\n",
    "        print(f\"   One-hot encoded: {low_card}\")\n",
    "    \n",
    "    # Frequency encode high cardinality\n",
    "    high_card = [col for col in categorical_cols if X[col].nunique() >= 10]\n",
    "    for col in high_card:\n",
    "        freq_map = X[col].value_counts(normalize=True).to_dict()\n",
    "        X[f'{col}_frequency'] = X[col].map(freq_map)\n",
    "        X = X.drop(col, axis=1)\n",
    "        print(f\"   Frequency encoded: {col}\")\n",
    "    \n",
    "    # Remove redundant features\n",
    "    redundant = ['monthly_income']\n",
    "    X = X.drop([col for col in redundant if col in X.columns], axis=1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete!\")\n",
    "    print(f\"   Final features: {X.shape[1]}\")\n",
    "    print(f\"   Ready for modeling\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# ==================== SKLEARN GRADIENT BOOSTING ====================\n",
    "\n",
    "def train_sklearn_gb(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train Sklearn's GradientBoostingClassifier\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SKLEARN GRADIENT BOOSTING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate sample weights for imbalance\n",
    "    n_samples = len(y_train)\n",
    "    n_defaults = (y_train == 1).sum()\n",
    "    n_no_defaults = (y_train == 0).sum()\n",
    "    \n",
    "    sample_weights = np.ones(n_samples)\n",
    "    sample_weights[y_train == 1] = n_samples / (2 * n_defaults)\n",
    "    sample_weights[y_train == 0] = n_samples / (2 * n_no_defaults)\n",
    "    \n",
    "    print(f\"\\nUsing sample weights to handle imbalance\")\n",
    "    print(f\"   Default weight: {sample_weights[y_train == 1][0]:.2f}\")\n",
    "    print(f\"   No default weight: {sample_weights[y_train == 0][0]:.2f}\")\n",
    "    \n",
    "    # Model parameters\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=500,           # Number of boosting stages\n",
    "        learning_rate=0.05,         # Learning rate\n",
    "        max_depth=7,                # Max depth of trees\n",
    "        min_samples_split=20,       # Min samples to split node\n",
    "        min_samples_leaf=10,        # Min samples per leaf\n",
    "        subsample=0.8,              # Fraction of samples for training each tree\n",
    "        max_features='sqrt',        # Number of features for best split\n",
    "        validation_fraction=0.1,    # Fraction for early stopping\n",
    "        n_iter_no_change=50,        # Early stopping rounds\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[1] Training Sklearn GradientBoosting...\")\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    print(f\"   Training stopped at iteration: {model.n_estimators_}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n[2] Model Evaluation:\")\n",
    "    \n",
    "    y_train_pred = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred)\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"   Training AUC:   {train_auc:.4f}\")\n",
    "    print(f\"   Test AUC:       {test_auc:.4f}\")\n",
    "    print(f\"   Overfitting:    {train_auc - test_auc:.4f}\")\n",
    "    \n",
    "    # Competition scoring\n",
    "    if test_auc >= 0.85:\n",
    "        grade = \"üèÜ EXCELLENT (25/25 points)\"\n",
    "    elif test_auc >= 0.80:\n",
    "        grade = \"‚úÖ VERY GOOD (23-24/25 points)\"\n",
    "    elif test_auc >= 0.75:\n",
    "        grade = \"‚úÖ GOOD (20-22/25 points)\"\n",
    "    elif test_auc >= 0.60:\n",
    "        grade = \"‚ö†Ô∏è  PASSING (15-19/25 points)\"\n",
    "    else:\n",
    "        grade = \"‚ùå DISQUALIFIED (<15 points)\"\n",
    "    \n",
    "    print(f\"\\n   Competition Score: {grade}\")\n",
    "    \n",
    "    return model, test_auc\n",
    "\n",
    "# ==================== CROSS-VALIDATION ====================\n",
    "\n",
    "def cross_validate_gb(X, y):\n",
    "    \"\"\"\n",
    "    5-fold stratified cross-validation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"5-FOLD CROSS-VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=50,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f\"\\nPerforming 5-fold stratified cross-validation...\")\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X, y,\n",
    "        cv=skf,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCV AUC Scores by Fold:\")\n",
    "    for i, score in enumerate(cv_scores, 1):\n",
    "        print(f\"   Fold {i}: {score:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Mean CV AUC: {cv_scores.mean():.4f}\")\n",
    "    print(f\"   Std Dev:     {cv_scores.std():.4f}\")\n",
    "    print(f\"   Min AUC:     {cv_scores.min():.4f}\")\n",
    "    print(f\"   Max AUC:     {cv_scores.max():.4f}\")\n",
    "    print(f\"   95% CI:      [{cv_scores.mean() - 1.96*cv_scores.std():.4f}, \"\n",
    "          f\"{cv_scores.mean() + 1.96*cv_scores.std():.4f}]\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# ==================== FEATURE IMPORTANCE ====================\n",
    "\n",
    "def plot_feature_importance(model, X, top_n=20):\n",
    "    \"\"\"\n",
    "    Display feature importance\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"TOP {top_n} MOST IMPORTANT FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{feature_importance.head(top_n).to_string(index=False)}\")\n",
    "        \n",
    "        return feature_importance\n",
    "    else:\n",
    "        print(\"\\nModel doesn't have feature_importances_ attribute\")\n",
    "        return None\n",
    "\n",
    "# ==================== ENSEMBLE OF GRADIENT BOOSTERS ====================\n",
    "\n",
    "def train_ensemble_gb(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Ensemble of multiple gradient boosting algorithms\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENSEMBLE: XGBoost + LightGBM + CatBoost + Sklearn GB\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    from xgboost import XGBClassifier\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from catboost import CatBoostClassifier\n",
    "    \n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    # Identify categorical features for CatBoost\n",
    "    cat_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\n[1] Training individual models...\")\n",
    "    \n",
    "    # XGBoost\n",
    "    print(\"\\n   Training XGBoost...\")\n",
    "    xgb_model = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=42,\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    xgb_pred = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    xgb_auc = roc_auc_score(y_test, xgb_pred)\n",
    "    print(f\"   XGBoost AUC: {xgb_auc:.4f}\")\n",
    "    \n",
    "    # LightGBM\n",
    "    print(\"\\n   Training LightGBM...\")\n",
    "    lgb_model = LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n",
    "                  callbacks=[])\n",
    "    lgb_pred = lgb_model.predict_proba(X_test)[:, 1]\n",
    "    lgb_auc = roc_auc_score(y_test, lgb_pred)\n",
    "    print(f\"   LightGBM AUC: {lgb_auc:.4f}\")\n",
    "    \n",
    "    # CatBoost\n",
    "    print(\"\\n   Training CatBoost...\")\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        learning_rate=0.03,\n",
    "        depth=7,\n",
    "        auto_class_weights='Balanced',\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    cat_model.fit(X_train, y_train, eval_set=(X_test, y_test), \n",
    "                  use_best_model=True, cat_features=cat_features)\n",
    "    cat_pred = cat_model.predict_proba(X_test)[:, 1]\n",
    "    cat_auc = roc_auc_score(y_test, cat_pred)\n",
    "    print(f\"   CatBoost AUC: {cat_auc:.4f}\")\n",
    "    \n",
    "    # Sklearn GB\n",
    "    print(\"\\n   Training Sklearn GradientBoosting...\")\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=7,\n",
    "        subsample=0.8,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    gb_pred = gb_model.predict_proba(X_test)[:, 1]\n",
    "    gb_auc = roc_auc_score(y_test, gb_pred)\n",
    "    print(f\"   Sklearn GB AUC: {gb_auc:.4f}\")\n",
    "    \n",
    "    # Ensemble predictions (weighted average)\n",
    "    print(f\"\\n[2] Creating ensemble...\")\n",
    "    \n",
    "    # Weight by performance\n",
    "    total_auc = xgb_auc + lgb_auc + cat_auc + gb_auc\n",
    "    w_xgb = xgb_auc / total_auc\n",
    "    w_lgb = lgb_auc / total_auc\n",
    "    w_cat = cat_auc / total_auc\n",
    "    w_gb = gb_auc / total_auc\n",
    "    \n",
    "    ensemble_pred = (w_xgb * xgb_pred + \n",
    "                     w_lgb * lgb_pred + \n",
    "                     w_cat * cat_pred + \n",
    "                     w_gb * gb_pred)\n",
    "    \n",
    "    ensemble_auc = roc_auc_score(y_test, ensemble_pred)\n",
    "    \n",
    "    print(f\"\\n[3] Ensemble Results:\")\n",
    "    print(f\"   Weights: XGB={w_xgb:.3f}, LGB={w_lgb:.3f}, CAT={w_cat:.3f}, GB={w_gb:.3f}\")\n",
    "    print(f\"   Ensemble AUC: {ensemble_auc:.4f}\")\n",
    "    print(f\"   Improvement: {ensemble_auc - max(xgb_auc, lgb_auc, cat_auc, gb_auc):+.4f}\")\n",
    "    \n",
    "    models = {\n",
    "        'xgboost': xgb_model,\n",
    "        'lightgbm': lgb_model,\n",
    "        'catboost': cat_model,\n",
    "        'sklearn_gb': gb_model\n",
    "    }\n",
    "    \n",
    "    return models, ensemble_auc\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "\n",
    "def main_gradient_boosting_pipeline(df):\n",
    "    \"\"\"\n",
    "    Complete Gradient Boosting pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GRADIENT BOOSTING CREDIT DEFAULT PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Preprocess\n",
    "    X, y = preprocess_for_gradient_boosting(df)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(\"DATA SPLIT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nTraining: {len(X_train):,} samples ({y_train.mean():.2%} default)\")\n",
    "    print(f\"Test:     {len(X_test):,} samples ({y_test.mean():.2%} default)\")\n",
    "    \n",
    "    # Train Sklearn GradientBoosting\n",
    "    model, test_auc = train_sklearn_gb(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_validate_gb(X_train, y_train)\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = plot_feature_importance(model, X_train, top_n=20)\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble_models, ensemble_auc = train_ensemble_gb(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n‚úÖ Sklearn GB Test AUC:  {test_auc:.4f}\")\n",
    "    print(f\"‚úÖ Ensemble AUC:         {ensemble_auc:.4f}\")\n",
    "    print(f\"‚úÖ Mean CV AUC:          {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "    print(f\"‚úÖ Total Features:       {X_train.shape[1]}\")\n",
    "    \n",
    "    best_auc = max(test_auc, ensemble_auc)\n",
    "    if best_auc >= 0.80:\n",
    "        print(f\"\\nüéØ TARGET ACHIEVED! Ready for competition.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  AUC below 80%. Try hyperparameter tuning.\")\n",
    "    \n",
    "    return model, ensemble_models, importance_df\n",
    "\n",
    "# ==================== USAGE ====================\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Run complete pipeline\n",
    "data = pd.read_csv('/Users/izzatillo_khazratov/Desktop/cbu-coding-challenge/data/final.csv')\n",
    "\n",
    "model, ensemble_models, importance = main_gradient_boosting_pipeline(data)\n",
    "\"\"\"\n",
    "# Make predictions with ensemble\n",
    "X_new_preprocessed, _ = preprocess_for_gradient_boosting(X_new)\n",
    "predictions = (\n",
    "    ensemble_models['xgboost'].predict_proba(X_new_preprocessed)[:, 1] * 0.3 +\n",
    "    ensemble_models['lightgbm'].predict_proba(X_new_preprocessed)[:, 1] * 0.3 +\n",
    "    ensemble_models['catboost'].predict_proba(X_new_preprocessed)[:, 1] * 0.3 +\n",
    "    ensemble_models['sklearn_gb'].predict_proba(X_new_preprocessed)[:, 1] * 0.1\n",
    ")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api-visn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
